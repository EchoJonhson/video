#!/usr/bin/env python3
"""
éŸ³é¢‘åˆ‡ç‰‡å·¥å…·

ä½¿ç”¨ VAD (Voice Activity Detection) å°†é•¿éŸ³é¢‘æ–‡ä»¶åˆ‡åˆ†ä¸ºçŸ­ç‰‡æ®µ
æ”¯æŒå¤šç§éŸ³é¢‘æ ¼å¼è¾“å…¥ï¼Œè¾“å‡ºä¸º 16kHz å•å£°é“ WAV æ ¼å¼

ä½¿ç”¨æ–¹æ³•ï¼š
    python audio_slicer.py --input_audio long_audio.wav --output_dir segments/
"""

import os
import sys
import json
import argparse
import subprocess
from pathlib import Path
import time


class AudioSlicer:
    def __init__(self, output_dir="segments", min_speech_duration_ms=1000, 
                 max_speech_duration_s=30, min_silence_duration_ms=500):
        self.output_dir = Path(output_dir)
        self.min_speech_duration_ms = min_speech_duration_ms
        self.max_speech_duration_s = max_speech_duration_s
        self.min_silence_duration_ms = min_silence_duration_ms
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def check_dependencies(self):
        """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
        print("ğŸ” æ£€æŸ¥ä¾èµ–...")
        
        # æ£€æŸ¥ ffmpeg
        try:
            subprocess.run(["ffmpeg", "-version"], capture_output=True, check=True)
            print("âœ… ffmpeg å·²å®‰è£…")
        except (subprocess.CalledProcessError, FileNotFoundError):
            print("âŒ ffmpeg æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£… ffmpeg")
            return False
        
        # æ£€æŸ¥ torch å’Œ torchaudio
        try:
            import torch
            import torchaudio
            print("âœ… torch å’Œ torchaudio å·²å®‰è£…")
        except ImportError:
            print("âŒ torch æˆ– torchaudio æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install torch torchaudio")
            return False
        
        # æ£€æŸ¥ silero-vad
        try:
            import silero_vad
            print("âœ… silero-vad å·²å®‰è£…")
        except ImportError:
            print("âŒ silero-vad æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install silero-vad")
            return False
        
        return True
    
    def prepare_audio(self, input_path, target_sample_rate=16000):
        """å‡†å¤‡éŸ³é¢‘ï¼šè½¬æ¢ä¸º 16kHz å•å£°é“ WAV æ ¼å¼"""
        print(f"ğŸµ å‡†å¤‡éŸ³é¢‘: {input_path}")
        
        input_path = Path(input_path)
        output_path = self.output_dir / "prepared_audio.wav"
        
        # ä½¿ç”¨ ffmpeg è½¬æ¢éŸ³é¢‘
        cmd = [
            "ffmpeg", "-i", str(input_path),
            "-ar", str(target_sample_rate),
            "-ac", "1",
            "-acodec", "pcm_s16le",
            "-f", "wav",
            "-y",  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
            str(output_path)
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            print(f"âœ… éŸ³é¢‘å‡†å¤‡å®Œæˆ: {output_path}")
            
            # è·å–éŸ³é¢‘ä¿¡æ¯
            info_cmd = [
                "ffprobe", "-v", "quiet", "-print_format", "json", "-show_format", "-show_streams",
                str(output_path)
            ]
            info_result = subprocess.run(info_cmd, capture_output=True, text=True, check=True)
            audio_info = json.loads(info_result.stdout)
            
            duration = float(audio_info['format']['duration'])
            print(f"ğŸ“Š éŸ³é¢‘æ—¶é•¿: {self.format_time(duration)}")
            
            return output_path, duration
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ éŸ³é¢‘è½¬æ¢å¤±è´¥: {e.stderr}")
            return None, None
    
    def segment_audio_with_vad(self, audio_path, duration):
        """ä½¿ç”¨ VAD åˆ‡ç‰‡éŸ³é¢‘"""
        print("âœ‚ï¸ ä½¿ç”¨ VAD åˆ‡ç‰‡éŸ³é¢‘...")
        print(f"ğŸ“‹ VAD å‚æ•°:")
        print(f"  - æœ€å°è¯­éŸ³æ®µé•¿åº¦: {self.min_speech_duration_ms}ms")
        print(f"  - æœ€å¤§è¯­éŸ³æ®µé•¿åº¦: {self.max_speech_duration_s}s")
        print(f"  - æœ€å°é™éŸ³é—´éš”: {self.min_silence_duration_ms}ms")
        
        try:
            # å¯¼å…¥å¿…è¦çš„åº“
            import torch
            import torchaudio
            from silero_vad import load_silero_vad, read_audio, get_speech_timestamps
            
            # åŠ è½½ VAD æ¨¡å‹
            print("ğŸ”„ åŠ è½½ VAD æ¨¡å‹...")
            model = load_silero_vad()
            print("âœ… VAD æ¨¡å‹åŠ è½½å®Œæˆ")
            
            # è¯»å–éŸ³é¢‘
            print("ğŸ“– è¯»å–éŸ³é¢‘æ–‡ä»¶...")
            wav = read_audio(str(audio_path))
            print(f"âœ… éŸ³é¢‘è¯»å–å®Œæˆï¼Œé‡‡æ ·ç‚¹æ•°: {len(wav)}")
            
            # è·å–è¯­éŸ³æ—¶é—´æˆ³
            print("ğŸ” æ£€æµ‹è¯­éŸ³æ´»åŠ¨...")
            start_time = time.time()
            
            speech_timestamps = get_speech_timestamps(
                wav, model, 
                return_seconds=True,
                min_speech_duration_ms=self.min_speech_duration_ms,
                max_speech_duration_s=self.max_speech_duration_s,
                min_silence_duration_ms=self.min_silence_duration_ms
            )
            
            vad_time = time.time() - start_time
            print(f"âœ… VAD æ£€æµ‹å®Œæˆ (è€—æ—¶: {vad_time:.2f}s)")
            print(f"ğŸ“Š æ£€æµ‹åˆ° {len(speech_timestamps)} ä¸ªè¯­éŸ³æ®µ")
            
            if not speech_timestamps:
                print("âš ï¸ è­¦å‘Š: æœªæ£€æµ‹åˆ°ä»»ä½•è¯­éŸ³æ®µ")
                return []
            
            # è®¡ç®—è¦†ç›–ç‡
            total_speech_duration = sum(seg['end'] - seg['start'] for seg in speech_timestamps)
            coverage = (total_speech_duration / duration) * 100
            print(f"ğŸ“ˆ è¯­éŸ³è¦†ç›–ç‡: {coverage:.1f}% ({self.format_time(total_speech_duration)}/{self.format_time(duration)})")
            
            # ä¿å­˜åˆ†æ®µä¿¡æ¯
            segments_info = []
            
            # åˆ‡ç‰‡éŸ³é¢‘
            print("\nâœ‚ï¸ å¼€å§‹åˆ‡ç‰‡éŸ³é¢‘...")
            for i, segment in enumerate(speech_timestamps):
                start_time_seg = segment['start']
                end_time_seg = segment['end']
                duration_seg = end_time_seg - start_time_seg
                
                # ç”Ÿæˆåˆ†æ®µæ–‡ä»¶å
                segment_filename = f"segment_{i:03d}.wav"
                segment_path = self.output_dir / segment_filename
                
                # ä½¿ç”¨ ffmpeg åˆ‡ç‰‡
                cmd = [
                    "ffmpeg", "-i", str(audio_path),
                    "-ss", str(start_time_seg),
                    "-t", str(duration_seg),
                    "-acodec", "copy",
                    "-y",
                    str(segment_path)
                ]
                
                try:
                    subprocess.run(cmd, capture_output=True, check=True)
                    
                    # è®°å½•åˆ†æ®µä¿¡æ¯
                    segment_info = {
                        "id": i,
                        "filename": segment_filename,
                        "start_time": start_time_seg,
                        "end_time": end_time_seg,
                        "duration": duration_seg,
                        "file_path": str(segment_path)
                    }
                    segments_info.append(segment_info)
                    
                    print(f"  âœ… åˆ†æ®µ {i:03d}: {self.format_time(start_time_seg)} - {self.format_time(end_time_seg)} ({self.format_time(duration_seg)})")
                    
                except subprocess.CalledProcessError as e:
                    print(f"  âŒ åˆ†æ®µ {i:03d} åˆ‡ç‰‡å¤±è´¥: {e}")
                    continue
            
            # ä¿å­˜åˆ†æ®µä¿¡æ¯åˆ° JSON æ–‡ä»¶
            segments_json_path = self.output_dir / "segments.json"
            with open(segments_json_path, 'w', encoding='utf-8') as f:
                json.dump({
                    "total_segments": len(segments_info),
                    "total_duration": duration,
                    "speech_duration": total_speech_duration,
                    "coverage_percent": coverage,
                    "vad_parameters": {
                        "min_speech_duration_ms": self.min_speech_duration_ms,
                        "max_speech_duration_s": self.max_speech_duration_s,
                        "min_silence_duration_ms": self.min_silence_duration_ms
                    },
                    "segments": segments_info
                }, f, ensure_ascii=False, indent=2)
            
            print(f"\nâœ… éŸ³é¢‘åˆ‡ç‰‡å®Œæˆ!")
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  - æ€»åˆ†æ®µæ•°: {len(segments_info)}")
            print(f"  - åŸå§‹æ—¶é•¿: {self.format_time(duration)}")
            print(f"  - è¯­éŸ³æ—¶é•¿: {self.format_time(total_speech_duration)}")
            print(f"  - è¦†ç›–ç‡: {coverage:.1f}%")
            print(f"ğŸ“„ åˆ†æ®µä¿¡æ¯ä¿å­˜è‡³: {segments_json_path}")
            print(f"ğŸ“ åˆ†æ®µæ–‡ä»¶ä¿å­˜è‡³: {self.output_dir}")
            
            return segments_info
            
        except Exception as e:
            print(f"âŒ éŸ³é¢‘åˆ‡ç‰‡å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
    
    def format_time(self, seconds):
        """æ ¼å¼åŒ–æ—¶é—´ä¸º HH:MM:SS æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"
    
    def slice_audio(self, input_audio):
        """å®Œæ•´çš„éŸ³é¢‘åˆ‡ç‰‡æµç¨‹"""
        print("âœ‚ï¸ éŸ³é¢‘åˆ‡ç‰‡å·¥å…·")
        print("=" * 50)
        
        # æ£€æŸ¥ä¾èµ–
        if not self.check_dependencies():
            return False
        
        # å‡†å¤‡éŸ³é¢‘
        print("\nğŸ”¹ å‡†å¤‡éŸ³é¢‘")
        prepared_audio, duration = self.prepare_audio(input_audio)
        if not prepared_audio:
            return False
        
        # åˆ‡ç‰‡éŸ³é¢‘
        print("\nğŸ”¹ VAD åˆ‡ç‰‡")
        segments_info = self.segment_audio_with_vad(prepared_audio, duration)
        if segments_info is None:
            return False
        
        print("\n" + "=" * 50)
        print("âœ… éŸ³é¢‘åˆ‡ç‰‡å®Œæˆ!")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        return True


def main():
    parser = argparse.ArgumentParser(description="éŸ³é¢‘åˆ‡ç‰‡å·¥å…· - ä½¿ç”¨ VAD å°†é•¿éŸ³é¢‘åˆ‡åˆ†ä¸ºçŸ­ç‰‡æ®µ")
    parser.add_argument('--input_audio', type=str, required=True, help="è¾“å…¥éŸ³é¢‘/è§†é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument('--output_dir', type=str, default='segments', help="è¾“å‡ºç›®å½•")
    parser.add_argument('--min_speech_duration_ms', type=int, default=1000, help="æœ€å°è¯­éŸ³æ®µé•¿åº¦(æ¯«ç§’)")
    parser.add_argument('--max_speech_duration_s', type=int, default=30, help="æœ€å¤§è¯­éŸ³æ®µé•¿åº¦(ç§’)")
    parser.add_argument('--min_silence_duration_ms', type=int, default=500, help="æœ€å°é™éŸ³é—´éš”(æ¯«ç§’)")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not Path(args.input_audio).exists():
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_audio}")
        return
    
    # åˆ›å»ºåˆ‡ç‰‡å™¨
    slicer = AudioSlicer(
        output_dir=args.output_dir,
        min_speech_duration_ms=args.min_speech_duration_ms,
        max_speech_duration_s=args.max_speech_duration_s,
        min_silence_duration_ms=args.min_silence_duration_ms
    )
    
    # æ‰§è¡Œåˆ‡ç‰‡
    success = slicer.slice_audio(args.input_audio)
    
    if success:
        print("\nğŸ‰ åˆ‡ç‰‡æˆåŠŸå®Œæˆ!")
    else:
        print("\nâŒ åˆ‡ç‰‡å¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()