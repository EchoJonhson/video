<div align="center">
<h1>FireRedASR: å¼€æºå·¥ä¸šçº§
<br>
è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹</h1>

</div>

[[è®ºæ–‡]](https://arxiv.org/pdf/2501.14350)
[[æ¨¡å‹]](https://huggingface.co/fireredteam)
[[åšå®¢]](https://fireredteam.github.io/demos/firered_asr/)

FireRedASR æ˜¯ä¸€ç³»åˆ—å¼€æºå·¥ä¸šçº§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œæ”¯æŒæ™®é€šè¯ã€ä¸­å›½æ–¹è¨€å’Œè‹±è¯­ï¼Œåœ¨å…¬å¼€çš„æ™®é€šè¯ ASR åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„æœ€ä½³æ°´å¹³ï¼ˆSOTAï¼‰ï¼ŒåŒæ—¶è¿˜æä¾›äº†å‡ºè‰²çš„æ­Œè¯è¯†åˆ«èƒ½åŠ›ã€‚


## ğŸ”¥ æœ€æ–°æ¶ˆæ¯
- [2025/02/17] æˆ‘ä»¬å‘å¸ƒäº† [FireRedASR-LLM-L](https://huggingface.co/fireredteam/FireRedASR-LLM-L/tree/main) æ¨¡å‹æƒé‡ã€‚
- [2025/01/24] æˆ‘ä»¬å‘å¸ƒäº† [æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/pdf/2501.14350)ã€[åšå®¢](https://fireredteam.github.io/demos/firered_asr/) å’Œ [FireRedASR-AED-L](https://huggingface.co/fireredteam/FireRedASR-AED-L/tree/main) æ¨¡å‹æƒé‡ã€‚


## æ–¹æ³•

FireRedASR æ—¨åœ¨æ»¡è¶³å„ç§åº”ç”¨åœºæ™¯ä¸­å¯¹å“è¶Šæ€§èƒ½å’Œæœ€ä¼˜æ•ˆç‡çš„å¤šæ ·åŒ–éœ€æ±‚ã€‚å®ƒåŒ…å«ä¸¤ä¸ªå˜ä½“ï¼š
- FireRedASR-LLMï¼šæ—¨åœ¨å®ç°æœ€å…ˆè¿›ï¼ˆSOTAï¼‰çš„æ€§èƒ½ï¼Œå¹¶æ”¯æŒæ— ç¼çš„ç«¯åˆ°ç«¯è¯­éŸ³äº¤äº’ã€‚å®ƒé‡‡ç”¨ç¼–ç å™¨-é€‚é…å™¨-LLMæ¡†æ¶ï¼Œå……åˆ†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ã€‚
- FireRedASR-AEDï¼šæ—¨åœ¨å¹³è¡¡é«˜æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ï¼Œå¹¶ä½œä¸ºåŸºäºLLMçš„è¯­éŸ³æ¨¡å‹ä¸­çš„æœ‰æ•ˆè¯­éŸ³è¡¨ç¤ºæ¨¡å—ã€‚å®ƒä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨-è§£ç å™¨ï¼ˆAEDï¼‰æ¶æ„ã€‚

![Model](/assets/FireRedASR_model.png)


## è¯„ä¼°
ç»“æœä»¥ä¸­æ–‡çš„å­—ç¬¦é”™è¯¯ç‡ï¼ˆCER%ï¼‰å’Œè‹±æ–‡çš„è¯é”™è¯¯ç‡ï¼ˆWER%ï¼‰æŠ¥å‘Šã€‚

### å…¬å¼€æ™®é€šè¯ASRåŸºå‡†æµ‹è¯•è¯„ä¼°
| Model            | #Params | aishell1 | aishell2 | ws\_net  | ws\_meeting | Average-4 |
|:----------------:|:-------:|:--------:|:--------:|:--------:|:-----------:|:---------:|
| FireRedASR-LLM   | 8.3B | 0.76 | 2.15 | 4.60 | 4.67 | 3.05 |
| FireRedASR-AED   | 1.1B | 0.55 | 2.52 | 4.88 | 4.76 | 3.18 |
| Seed-ASR         | 12B+ | 0.68 | 2.27 | 4.66 | 5.69 | 3.33 |
| Qwen-Audio       | 8.4B | 1.30 | 3.10 | 9.50 | 10.87 | 6.19 |
| SenseVoice-L     | 1.6B | 2.09 | 3.04 | 6.01 | 6.73 | 4.47 |
| Whisper-Large-v3 | 1.6B | 5.14 | 4.96 | 10.48 | 18.87 | 9.86 |
| Paraformer-Large | 0.2B | 1.68 | 2.85 | 6.74 | 6.97 | 4.56 |

`ws` means WenetSpeech.

### å…¬å¼€ä¸­å›½æ–¹è¨€å’Œè‹±è¯­ASRåŸºå‡†æµ‹è¯•è¯„ä¼°
|Test Set       | KeSpeech | LibriSpeech test-clean | LibriSpeech test-other  |
| :------------:| :------: | :--------------------: | :----------------------:|
|FireRedASR-LLM | 3.56 | 1.73 | 3.67 |
|FireRedASR-AED | 4.48 | 1.93 | 4.44 |
|Previous SOTA Results | 6.70 | 1.82 | 3.50 |


## ä½¿ç”¨æ–¹æ³•

**æ³¨æ„ï¼šç”±äºæ¨¡å‹æ–‡ä»¶è¿‡å¤§ï¼Œæœ¬ä»“åº“ä¸åŒ…å«é¢„è®­ç»ƒæ¨¡å‹ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼š**

ä» [huggingface](https://huggingface.co/fireredteam) ä¸‹è½½æ¨¡å‹æ–‡ä»¶å¹¶å°†å…¶æ”¾ç½®åœ¨ `pretrained_models` æ–‡ä»¶å¤¹ä¸­ã€‚

å¦‚æœæ‚¨æƒ³ä½¿ç”¨ `FireRedASR-LLM-L`ï¼Œè¿˜éœ€è¦ä¸‹è½½ [Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct) å¹¶å°†å…¶æ”¾ç½®åœ¨ `pretrained_models` æ–‡ä»¶å¤¹ä¸­ã€‚ç„¶åè¿›å…¥ `FireRedASR-LLM-L` æ–‡ä»¶å¤¹å¹¶è¿è¡Œ `$ ln -s ../Qwen2-7B-Instruct`


### ç¯å¢ƒè®¾ç½®
åˆ›å»º Python ç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
```bash
$ git clone https://github.com/EchoJonhson/video.git
$ cd video
$ conda create --name fireredasr python=3.10
$ conda activate fireredasr
$ pip install -r requirements.txt
```

è®¾ç½® Linux PATH å’Œ PYTHONPATH
```bash
$ export PATH=$PWD/fireredasr/:$PWD/fireredasr/utils/:$PATH
$ export PYTHONPATH=$PWD/:$PYTHONPATH
```

### æ ¼å¼è½¬æ¢

**éŸ³é¢‘æ ¼å¼è½¬æ¢** - å°†éŸ³é¢‘è½¬æ¢ä¸º 16kHz 16-bit PCM æ ¼å¼
```bash
ffmpeg -i input_audio -ar 16000 -ac 1 -acodec pcm_s16le -f wav output.wav
```

**ä»è§†é¢‘æå–éŸ³é¢‘** - ä»è§†é¢‘æ–‡ä»¶æå–éŸ³é¢‘ï¼ˆå¯é€‰ï¼ŒFireRedASRå¯è‡ªåŠ¨å¤„ç†ï¼‰
```bash
ffmpeg -i input_video.mp4 -ar 16000 -ac 1 -acodec pcm_s16le -f wav output.wav
```

### å¿«é€Ÿå¼€å§‹

**å¤„ç†éŸ³é¢‘æ–‡ä»¶**
```bash
$ cd examples
$ bash inference_fireredasr_aed.sh
$ bash inference_fireredasr_llm.sh
```

**å¤„ç†è§†é¢‘æ–‡ä»¶**
```bash
$ cd examples
$ bash inference_video.sh                    # Shellè„šæœ¬ç¤ºä¾‹
$ python video_processing_example.py         # Pythonç¤ºä¾‹
```

### å‘½ä»¤è¡Œä½¿ç”¨

**éŸ³é¢‘æ–‡ä»¶å¤„ç†ï¼ˆå‘åå…¼å®¹ï¼‰**
```bash
$ speech2text.py --help
$ speech2text.py --wav_path examples/wav/BAC009S0764W0121.wav --asr_type "aed" --model_dir pretrained_models/FireRedASR-AED-L
$ speech2text.py --wav_path examples/wav/BAC009S0764W0121.wav --asr_type "llm" --model_dir pretrained_models/FireRedASR-LLM-L
```

**è§†é¢‘æ–‡ä»¶å¤„ç†**
```bash
# å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶
$ speech2text.py --video_path video.mp4 --asr_type "aed" --model_dir pretrained_models/FireRedASR-AED-L
$ speech2text.py --input_path video.mp4 --asr_type "llm" --model_dir pretrained_models/FireRedASR-LLM-L

# æ‰¹é‡å¤„ç†è§†é¢‘ç›®å½•
$ speech2text.py --video_dir videos/ --asr_type "aed" --model_dir pretrained_models/FireRedASR-AED-L

# æ··åˆå¤„ç†éŸ³é¢‘å’Œè§†é¢‘
$ speech2text.py --input_dir media/ --asr_type "aed" --model_dir pretrained_models/FireRedASR-AED-L
$ speech2text.py --input_paths audio.wav video.mp4 --asr_type "aed" --model_dir pretrained_models/FireRedASR-AED-L
```

**æ”¯æŒçš„è§†é¢‘æ ¼å¼ï¼š** MP4, AVI, MOV, MKV, FLV, WMV  
**æ”¯æŒçš„éŸ³é¢‘æ ¼å¼ï¼š** WAV, MP3, FLAC, M4A, AAC, OGG

### Python ä½¿ç”¨ç¤ºä¾‹

**å¤„ç†éŸ³é¢‘æ–‡ä»¶**
```python
from fireredasr.models.fireredasr import FireRedAsr

batch_uttid = ["BAC009S0764W0121"]
batch_wav_path = ["examples/wav/BAC009S0764W0121.wav"]

# FireRedASR-AED
model = FireRedAsr.from_pretrained("aed", "pretrained_models/FireRedASR-AED-L")
results = model.transcribe(
    batch_uttid,
    batch_wav_path,
    {
        "use_gpu": 1,
        "beam_size": 3,
        "nbest": 1,
        "decode_max_len": 0,
        "softmax_smoothing": 1.25,
        "aed_length_penalty": 0.6,
        "eos_penalty": 1.0
    }
)
print(results)
```

**å¤„ç†è§†é¢‘æ–‡ä»¶**
```python
from fireredasr.models.fireredasr import FireRedAsr

# ç›´æ¥å¤„ç†è§†é¢‘æ–‡ä»¶ï¼ŒFireRedASRä¼šè‡ªåŠ¨æå–éŸ³é¢‘
batch_uttid = ["my_video"]
batch_video_path = ["path/to/video.mp4"]

# FireRedASR-LLM å¤„ç†è§†é¢‘
model = FireRedAsr.from_pretrained("llm", "pretrained_models/FireRedASR-LLM-L")
results = model.transcribe(
    batch_uttid,
    batch_video_path,
    {
        "use_gpu": 1,
        "beam_size": 3,
        "decode_max_len": 0,
        "decode_min_len": 0,
        "repetition_penalty": 3.0,
        "llm_length_penalty": 1.0,
        "temperature": 1.0
    }
)
print(results)

# å¤„ç†å®Œæˆåï¼Œä¸´æ—¶æ–‡ä»¶ä¼šè‡ªåŠ¨æ¸…ç†
model.feat_extractor.cleanup_temp_files()
```

**æ··åˆå¤„ç†éŸ³é¢‘å’Œè§†é¢‘**
```python
# å¯ä»¥åœ¨åŒä¸€æ‰¹æ¬¡ä¸­æ··åˆå¤„ç†ä¸åŒæ ¼å¼çš„æ–‡ä»¶
batch_uttid = ["audio_sample", "video_sample"]
batch_media_path = ["audio.wav", "video.mp4"]

model = FireRedAsr.from_pretrained("aed", "pretrained_models/FireRedASR-AED-L")
results = model.transcribe(batch_uttid, batch_media_path, config)
```

## ä½¿ç”¨æŠ€å·§
### æ‰¹é‡æŸæœç´¢
- åœ¨ä½¿ç”¨ FireRedASR-LLM è¿›è¡Œæ‰¹é‡æŸæœç´¢æ—¶ï¼Œè¯·ç¡®ä¿è¾“å…¥è¯­éŸ³çš„é•¿åº¦ç›¸ä¼¼ã€‚å¦‚æœè¯­éŸ³é•¿åº¦å·®å¼‚è¾ƒå¤§ï¼Œè¾ƒçŸ­çš„è¯­éŸ³å¯èƒ½ä¼šå‡ºç°é‡å¤é—®é¢˜ã€‚æ‚¨å¯ä»¥æŒ‰é•¿åº¦å¯¹æ•°æ®é›†è¿›è¡Œæ’åºï¼Œæˆ–è€…å°† `batch_size` è®¾ç½®ä¸º 1 æ¥é¿å…è¿™ä¸ªé—®é¢˜ã€‚

### è¾“å…¥é•¿åº¦é™åˆ¶
- FireRedASR-AED æ”¯æŒæœ€é•¿ 60 ç§’çš„éŸ³é¢‘è¾“å…¥ã€‚è¶…è¿‡ 60 ç§’çš„è¾“å…¥å¯èƒ½ä¼šå¯¼è‡´å¹»è§‰é—®é¢˜ï¼Œè¶…è¿‡ 200 ç§’çš„è¾“å…¥å°†è§¦å‘ä½ç½®ç¼–ç é”™è¯¯ã€‚
- FireRedASR-LLM æ”¯æŒæœ€é•¿ 30 ç§’çš„éŸ³é¢‘è¾“å…¥ã€‚æ›´é•¿è¾“å…¥çš„è¡Œä¸ºç›®å‰å°šä¸æ¸…æ¥šã€‚


## è‡´è°¢
æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®ï¼š
- [Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)
- [icefall/ASR_LLM](https://github.com/k2-fsa/icefall/tree/master/egs/speech_llm/ASR_LLM)
- [WeNet](https://github.com/wenet-e2e/wenet)
- [Speech-Transformer](https://github.com/kaituoxu/Speech-Transformer)


## å¼•ç”¨
```bibtex
@article{xu2025fireredasr,
  title={FireRedASR: Open-Source Industrial-Grade Mandarin Speech Recognition Models from Encoder-Decoder to LLM Integration},
  author={Xu, Kai-Tuo and Xie, Feng-Long and Tang, Xu and Hu, Yao},
  journal={arXiv preprint arXiv:2501.14350},
  year={2025}
}
```
