# ä¼˜åŒ–å»ºè®®è¡¥ä¸ - é’ˆå¯¹ long_video_transcribe.py çš„ç¬¬ 441-472 è¡Œ

# åŸä»£ç ï¼ˆç¬¬441-472è¡Œï¼‰çš„ä¼˜åŒ–ç‰ˆæœ¬ï¼š

if self.model_type == "llm":
    # å¯¼å…¥CPUä¼˜åŒ–é…ç½®
    from utils.cpu_optimization_config import CPUOptimizationConfig
    cpu_optimizer = CPUOptimizationConfig()
    
    # è·å–åŠ¨æ€ä¼˜åŒ–é…ç½®
    opt_config = cpu_optimizer.get_dynamic_config(segment_count, "llm")
    
    # LLM æ¨¡å‹æ™ºèƒ½å¤„ç†ç­–ç•¥
    gpu_assisted = self.smart_loader.strategy.get('gpu_role') in ['encoder_only', 'feature_extraction']
    
    if gpu_assisted:
        # GPUè¾…åŠ©æ¨¡å¼ä¸‹çš„ä¼˜åŒ–é…ç½®
        max_workers = opt_config["max_workers"]
        batch_size = opt_config["batch_size"]
        
        # å†…å­˜ä½¿ç”¨ä¼°ç®—
        memory_est = cpu_optimizer.estimate_memory_usage("llm", max_workers)
        
        print(f"ğŸš€ LLM GPUè¾…åŠ©æ¨¡å¼ä¼˜åŒ–:")
        print(f"   - åˆ†æ®µæ•°: {segment_count}")
        print(f"   - å¹¶è¡Œçº¿ç¨‹: {max_workers} (åŸ2ä¸ªï¼Œç°ä¼˜åŒ–ä¸º{max_workers}ä¸ª)")
        print(f"   - é¢„ä¼°å†…å­˜: {memory_est['total_gb']:.1f}GB / {memory_est['available_gb']:.1f}GB ({memory_est['usage_percent']:.1f}%)")
        print(f"   - CPUé…ç½®: i9-14900KF (24æ ¸32çº¿ç¨‹)")
        print("ğŸ“Œ ä¼˜åŒ–ç­–ç•¥: ç¼–ç å™¨åœ¨GPUï¼ŒLLMä¸»ä½“åœ¨CPUï¼Œä½¿ç”¨CPUäº²å’Œæ€§ä¼˜åŒ–")
        
        # è®¾ç½®CPUäº²å’Œæ€§ï¼ˆå¯é€‰ï¼‰
        if opt_config["cpu_affinity"]["enable"]:
            import os
            # ä¸ºå½“å‰è¿›ç¨‹è®¾ç½®CPUäº²å’Œæ€§
            os.sched_setaffinity(0, opt_config["cpu_affinity"]["llm_cores"])
            
        # å¯ç”¨é¢„è¯»å–ä¼˜åŒ–
        self.prefetch_segments = opt_config["memory_config"]["prefetch_segments"]
        
    else:
        # çº¯CPUæ¨¡å¼ä¿æŒåŸæœ‰ç­–ç•¥
        max_workers = 1
        batch_size = 1
        if segment_count <= 10:
            print(f"âš ï¸ LLM ä¸²è¡Œå¤„ç†: åˆ†æ®µæ•°è¾ƒå°‘({segment_count}ä¸ª)ï¼Œä½¿ç”¨ä¸²è¡Œå¤„ç†")
        else:
            print("âš ï¸ LLM çº¯CPUæ¨¡å¼ï¼Œä½¿ç”¨ä¸²è¡Œå¤„ç†ä»¥ç¡®ä¿ç¨³å®šæ€§")
            
else:
    # AED æ¨¡å‹ä¼˜åŒ–
    from utils.cpu_optimization_config import CPUOptimizationConfig
    cpu_optimizer = CPUOptimizationConfig()
    opt_config = cpu_optimizer.get_dynamic_config(segment_count, "aed")
    
    max_workers = opt_config["max_workers"]
    batch_size = opt_config["batch_size"]
    
    print(f"ğŸ”§ AED æ™ºèƒ½å¹¶è¡Œä¼˜åŒ–:")
    print(f"   - åˆ†æ®µæ•°: {segment_count}")
    print(f"   - å¹¶è¡Œçº¿ç¨‹: {max_workers}")
    print(f"   - æ‰¹å¤„ç†å¤§å°: {batch_size}")

print(f"ğŸ”§ æœ€ç»ˆå¤„ç†é…ç½®: {max_workers} çº¿ç¨‹, æ‰¹æ¬¡å¤§å°: {batch_size}")

# é¢å¤–ä¼˜åŒ–ï¼šæ·»åŠ é¢„è¯»å–çº¿ç¨‹æ± 
if hasattr(self, 'prefetch_segments') and self.prefetch_segments > 0:
    # åˆ›å»ºé¢„è¯»å–çº¿ç¨‹æ± 
    from concurrent.futures import ThreadPoolExecutor
    prefetch_executor = ThreadPoolExecutor(max_workers=2)
    
    def prefetch_audio_segments(start_idx, count):
        """é¢„è¯»å–éŸ³é¢‘æ®µåˆ°å†…å­˜"""
        for i in range(start_idx, min(start_idx + count, len(segments))):
            segment_path = segments_dir / segments[i]['file']
            if segment_path.exists():
                # é¢„åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                with open(segment_path, 'rb') as f:
                    audio_cache[i] = f.read()