#!/usr/bin/env python3
"""
FireRedASR é•¿è§†é¢‘è½¬æ–‡å­—æ‰¹é‡å¤„ç†ç³»ç»Ÿ

åŠŸèƒ½ï¼š
- è‡ªåŠ¨æ‰«æ Use/Input/ ä¸­çš„é•¿è§†é¢‘æ–‡ä»¶
- æ™ºèƒ½åˆ‡ç‰‡å¤„ç†ï¼ˆä½¿ç”¨ Silero VADï¼‰
- æ‰¹é‡ä½¿ç”¨ FireRedASR æ¨¡å‹è½¬å†™
- æ‹¼æ¥æˆå®Œæ•´æ–‡å­—ç¨¿å’Œå­—å¹•æ–‡ä»¶
- ç»“æœä¿å­˜åˆ° Use/Output/ æ–‡ä»¶å¤¹

ä½¿ç”¨æ–¹æ³•ï¼š
    python long_video_transcribe.py
    python long_video_transcribe.py --model_type llm
    python long_video_transcribe.py --max_duration 45 --min_silence 300
"""

import os
import sys
import json
import argparse
import subprocess
import shutil
import tempfile
from pathlib import Path
from datetime import datetime, timedelta
import time
import torch
import torchaudio
import requests
import zipfile

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from fireredasr.models.fireredasr import FireRedAsr
from utils.hardware_manager import get_hardware_manager
from utils.smart_model_loader import create_smart_loader
from utils.parallel_processor import AudioBatchProcessor
from fireredasr.utils.video_audio import is_video_file, is_audio_file
from fireredasr.utils.punctuation_restore import PunctuationRestorer
from fireredasr.utils.paragraph_segmentation import ParagraphSegmenter


class LongVideoTranscriber:
    def __init__(self):
        self.input_dir = Path("Use/Input")
        self.output_dir = Path("Use/Output")
        self.temp_dir = Path("Use/Output/temp_long_video")
        
        # VAD å‚æ•°
        self.min_speech_duration_ms = 1000
        self.max_speech_duration_s = 30
        self.min_silence_duration_ms = 500
        
        # æ¨¡å‹ç›¸å…³
        self.model_type = None
        self.model = None
        
        # æ”¯æŒçš„æ ¼å¼
        self.supported_video = {'.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv'}
        self.supported_audio = {'.wav', '.mp3', '.flac', '.m4a', '.aac', '.ogg'}
        
        # åˆå§‹åŒ–æ™ºèƒ½ç³»ç»Ÿ
        print("ğŸ”§ åˆå§‹åŒ–æ™ºèƒ½å¤„ç†ç³»ç»Ÿ...")
        self.hardware_manager = get_hardware_manager()
        self.smart_loader = create_smart_loader(self.hardware_manager)
        self.parallel_processor = None
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®
        force_cpu = os.environ.get('FIREREDASR_FORCE_CPU', '').lower() in ['1', 'true', 'yes']
        if force_cpu:
            print("âš ï¸ å¼ºåˆ¶ä½¿ç”¨ CPU æ¨¡å¼ (FIREREDASR_FORCE_CPU=1)")
            self.hardware_manager.strategy['name'] = 'cpu_primary'
            self.hardware_manager.strategy['use_gpu'] = False
        
        # æ‰“å°ç¡¬ä»¶é…ç½®
        self.hardware_manager.print_hardware_info()
        
        # æ ‡ç‚¹æ¢å¤ç›¸å…³
        self.enable_punctuation = True  # é»˜è®¤å¯ç”¨æ ‡ç‚¹æ¢å¤
        self.punctuation_restorer = None
        self.punctuation_model_dir = None
        self.punctuation_chunk_size = 256
        self.punctuation_stride = 128
        
        # åˆ†æ®µç›¸å…³
        self.enable_paragraph = False  # é»˜è®¤ä¸å¯ç”¨åˆ†æ®µ
        self.paragraph_segmenter = None
        self.paragraph_method = "rule"  # rule/semantic/hybrid
        self.min_paragraph_length = 50
        self.max_paragraph_length = 500
        
    def check_dependencies(self):
        """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
        print("ğŸ” æ£€æŸ¥ä¾èµ–...")
        
        # æ£€æŸ¥ ffmpeg
        try:
            subprocess.run(["ffmpeg", "-version"], capture_output=True, check=True)
            print("âœ… ffmpeg å·²å®‰è£…")
        except (subprocess.CalledProcessError, FileNotFoundError):
            print("âŒ ffmpeg æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£… ffmpeg")
            return False
        
        # æ£€æŸ¥ torchaudio
        try:
            import torchaudio
            print("âœ… torchaudio å·²å®‰è£…")
        except ImportError:
            print("âŒ torchaudio æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install torchaudio")
            return False
        
        return True
    
    def scan_long_media_files(self):
        """æ‰«æè¾“å…¥æ–‡ä»¶å¤¹ä¸­çš„é•¿åª’ä½“æ–‡ä»¶"""
        if not self.input_dir.exists():
            print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {self.input_dir}")
            return []
        
        media_files = []
        all_extensions = self.supported_audio | self.supported_video
        
        for file_path in self.input_dir.iterdir():
            if file_path.is_file() and file_path.suffix.lower() in all_extensions:
                # è·å–æ–‡ä»¶æ—¶é•¿ï¼ˆç²—ç•¥ä¼°è®¡ï¼‰
                file_size_mb = file_path.stat().st_size / 1024 / 1024
                # ä¸ºäº†æµ‹è¯•ï¼Œæš‚æ—¶é™ä½æ–‡ä»¶å¤§å°é™åˆ¶
                if file_size_mb > 0.1:  # å¤§äº0.1MBçš„æ–‡ä»¶ï¼ˆæµ‹è¯•ç”¨ï¼‰
                    media_files.append(file_path)
        
        return sorted(media_files)
    
    def display_files(self, files):
        """æ˜¾ç¤ºæ‰¾åˆ°çš„æ–‡ä»¶"""
        if not files:
            print("âŒ åœ¨ Use/Input/ æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°å¤§å‹åª’ä½“æ–‡ä»¶")
            print("æç¤ºï¼šé•¿è§†é¢‘å¤„ç†é€‚ç”¨äºå¤§äº10MBçš„éŸ³è§†é¢‘æ–‡ä»¶")
            return False
        
        print(f"\nğŸ“ åœ¨ Use/Input/ ä¸­æ‰¾åˆ° {len(files)} ä¸ªå¤§å‹åª’ä½“æ–‡ä»¶:")
        print("-" * 60)
        
        for i, file_path in enumerate(files, 1):
            file_size = file_path.stat().st_size / 1024 / 1024  # MB
            if is_video_file(str(file_path)):
                file_type = "ğŸ“¹ è§†é¢‘"
            else:
                file_type = "ğŸµ éŸ³é¢‘"
            
            print(f"{i:2d}. {file_type} | {file_path.name} ({file_size:.2f} MB)")
        
        print("-" * 60)
        return True
    
    def get_model_dir(self):
        """æ ¹æ®å‘½ä»¤è¡Œå‚æ•°è·å–æ¨¡å‹ç›®å½•"""
        if not self.model_type:
            print("âŒ æœªæŒ‡å®šæ¨¡å‹ç±»å‹ï¼Œè¯·ä½¿ç”¨ --model_type å‚æ•°")
            return None
        
        if self.model_type == "aed":
            model_dir = "pretrained_models/FireRedASR-AED-L"
            print("âœ… ä½¿ç”¨ FireRedASR-AED æ¨¡å‹ (å¿«é€Ÿ, é€‚åˆé•¿éŸ³é¢‘)")
        elif self.model_type == "llm":
            model_dir = "pretrained_models/FireRedASR-LLM-L"
            print("âœ… ä½¿ç”¨ FireRedASR-LLM æ¨¡å‹ (é«˜ç²¾åº¦, å¤„ç†è¾ƒæ…¢)")
        else:
            print(f"âŒ æœªçŸ¥æ¨¡å‹ç±»å‹: {self.model_type}")
            return None
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not Path(model_dir).exists():
            print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
            print("è¯·å…ˆä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œå‚è€ƒ step.md æ–‡æ¡£")
            return None
        
        return model_dir
    
    def select_model(self):
        """è®©ç”¨æˆ·é€‰æ‹©æ¨¡å‹"""
        print("\nğŸ¤– è¯·é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹:")
        print("1. FireRedASR-AED (å¿«é€Ÿ, é€‚åˆé•¿éŸ³é¢‘)")
        print("2. FireRedASR-LLM (é«˜ç²¾åº¦, å¤„ç†è¾ƒæ…¢)")
        
        while True:
            try:
                choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1 æˆ– 2): ").strip()
                if choice == "1":
                    self.model_type = "aed"
                    model_dir = "pretrained_models/FireRedASR-AED-L"
                    print("âœ… é€‰æ‹©äº† FireRedASR-AED æ¨¡å‹")
                    break
                elif choice == "2":
                    self.model_type = "llm"
                    model_dir = "pretrained_models/FireRedASR-LLM-L"
                    print("âœ… é€‰æ‹©äº† FireRedASR-LLM æ¨¡å‹")
                    break
                else:
                    print("âŒ æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ 1 æˆ– 2")
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                return None
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not Path(model_dir).exists():
            print(f"âŒ é”™è¯¯: æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
            print("è¯·ä» https://huggingface.co/fireredteam ä¸‹è½½æ¨¡å‹æ–‡ä»¶")
            return None
        
        return model_dir
    
    def prepare_audio(self, input_path, output_path):
        """å‡†å¤‡éŸ³é¢‘ï¼šè½¬æ¢ä¸º 16kHz å•å£°é“ WAV æ ¼å¼"""
        print(f"ğŸµ å‡†å¤‡éŸ³é¢‘: {input_path.name}")
        
        # ä½¿ç”¨ ffmpeg è½¬æ¢éŸ³é¢‘
        cmd = [
            "ffmpeg", "-i", str(input_path),
            "-ar", "16000",
            "-ac", "1",
            "-acodec", "pcm_s16le",
            "-f", "wav",
            "-y",  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
            str(output_path)
        ]
        
        try:
            subprocess.run(cmd, capture_output=True, text=True, check=True)
            print(f"âœ… éŸ³é¢‘å‡†å¤‡å®Œæˆ")
            return True
        except subprocess.CalledProcessError as e:
            print(f"âŒ éŸ³é¢‘è½¬æ¢å¤±è´¥: {e.stderr}")
            return False
    
    def load_silero_vad(self):
        """åŠ è½½ Silero VAD æ¨¡å‹"""
        print("ğŸ”„ åŠ è½½ VAD æ¨¡å‹...")
        
        # æ–¹æ³•1: ä½¿ç”¨ pip å®‰è£…çš„ silero-vad åŒ…ï¼ˆæ¨èï¼‰
        try:
            print("ğŸ“¦ å°è¯•ä½¿ç”¨ silero-vad åŒ…...")
            from silero_vad import load_silero_vad, get_speech_timestamps, read_audio
            
            model = load_silero_vad()
            
            # åˆ›å»ºå…¼å®¹çš„ save_audio å‡½æ•°
            def save_audio(path, tensor, sampling_rate):
                torchaudio.save(path, tensor, sampling_rate)
            
            print("âœ… VAD æ¨¡å‹åŠ è½½æˆåŠŸ (silero-vad åŒ…)")
            return model, get_speech_timestamps, read_audio, save_audio
            
        except ImportError as e:
            print(f"âŒ silero-vad åŒ…æœªå®‰è£…: {e}")
        except Exception as e:
            print(f"âŒ silero-vad åŒ…åŠ è½½å¤±è´¥: {str(e)}")
        
        # æ–¹æ³•2: å°è¯•ä» torch.hub åŠ è½½
        for attempt in range(2):
            try:
                print(f"ğŸ“ å°è¯•ä» torch.hub åŠ è½½ (å°è¯• {attempt + 1}/2)...")
                model, utils = torch.hub.load(
                    repo_or_dir='snakers4/silero-vad',
                    model='silero_vad',
                    force_reload=attempt > 0,
                    trust_repo=True
                )
                (get_speech_timestamps, save_audio, read_audio, 
                 VADIterator, collect_chunks) = utils
                
                print("âœ… VAD æ¨¡å‹åŠ è½½æˆåŠŸ (torch.hub)")
                return model, get_speech_timestamps, read_audio, save_audio
                
            except Exception as e:
                print(f"âŒ torch.hub åŠ è½½å¤±è´¥ (å°è¯• {attempt + 1}/2): {str(e)}")
                if attempt == 0:
                    time.sleep(3)
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
        raise Exception("âŒ VADæ¨¡å‹åŠ è½½å¤±è´¥ï¼è¯·ç¡®ä¿å·²å®‰è£… silero-vad: pip install silero-vad")
    
    def slice_audio_with_vad(self, audio_path, output_dir):
        """ä½¿ç”¨ VAD åˆ‡åˆ†éŸ³é¢‘"""
        print(f"âœ‚ï¸ å¼€å§‹åˆ‡åˆ†éŸ³é¢‘...")
        
        # åŠ è½½ VAD æ¨¡å‹
        vad_model, get_speech_timestamps, read_audio, save_audio = self.load_silero_vad()
        
        # è¯»å–éŸ³é¢‘
        wav = read_audio(str(audio_path))
        
        # è·å–è¯­éŸ³æ—¶é—´æˆ³  
        speech_timestamps = get_speech_timestamps(
            wav, 
            vad_model,
            threshold=0.5,
            sampling_rate=16000,
            min_speech_duration_ms=self.min_speech_duration_ms,
            min_silence_duration_ms=self.min_silence_duration_ms
        )
        
        # è½¬æ¢ä¸ºç§’ï¼ˆsilero-vadé»˜è®¤è¿”å›é‡‡æ ·ç‚¹ï¼Œéœ€è¦è½¬æ¢ï¼‰
        for ts in speech_timestamps:
            ts['start'] = ts['start'] / 16000.0
            ts['end'] = ts['end'] / 16000.0
        
        if not speech_timestamps:
            print("âŒ æ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³æ®µ")
            return []
        
        print(f"âœ… æ£€æµ‹åˆ° {len(speech_timestamps)} ä¸ªåˆå§‹è¯­éŸ³æ®µ")
        
        # åˆå¹¶å’Œåˆ‡åˆ†è¯­éŸ³æ®µ
        segments = []
        current_segment = None
        
        for timestamp in speech_timestamps:
            if current_segment is None:
                current_segment = {
                    'start': timestamp['start'],
                    'end': timestamp['end']
                }
            elif (timestamp['start'] - current_segment['end'] < 1.0 and 
                  timestamp['end'] - current_segment['start'] < self.max_speech_duration_s):
                # åˆå¹¶ç›¸è¿‘çš„æ®µ
                current_segment['end'] = timestamp['end']
            else:
                # ä¿å­˜å½“å‰æ®µï¼Œå¼€å§‹æ–°æ®µ
                segments.append(current_segment)
                current_segment = {
                    'start': timestamp['start'],
                    'end': timestamp['end']
                }
        
        if current_segment:
            segments.append(current_segment)
        
        print(f"âœ… åˆå¹¶åå¾—åˆ° {len(segments)} ä¸ªè¯­éŸ³æ®µ")
        
        # ä¿å­˜éŸ³é¢‘æ®µ
        segment_files = []
        waveform, sample_rate = torchaudio.load(str(audio_path))
        
        for i, segment in enumerate(segments):
            start_sample = int(segment['start'] * sample_rate)
            end_sample = int(segment['end'] * sample_rate)
            
            segment_waveform = waveform[:, start_sample:end_sample]
            segment_path = output_dir / f"segment_{i:03d}.wav"
            
            torchaudio.save(str(segment_path), segment_waveform, sample_rate)
            
            segment_files.append({
                'index': i,
                'file': segment_path.name,
                'start': segment['start'],
                'end': segment['end'],
                'duration': segment['end'] - segment['start']
            })
        
        # ä¿å­˜åˆ†æ®µä¿¡æ¯
        segments_info_path = output_dir / "segments.json"
        with open(segments_info_path, 'w', encoding='utf-8') as f:
            json.dump(segment_files, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… éŸ³é¢‘åˆ‡åˆ†å®Œæˆï¼Œå…± {len(segment_files)} ä¸ªç‰‡æ®µ")
        return segment_files
    
    def batch_transcribe(self, segments_dir, model_dir):
        """æ™ºèƒ½æ‰¹é‡è½¬å†™éŸ³é¢‘ç‰‡æ®µ"""
        print("\nğŸ¤ å¼€å§‹æ™ºèƒ½æ‰¹é‡è½¬å†™...")
        
        # éªŒè¯åˆ†æ®µç›®å½•å’Œæ–‡ä»¶
        if not segments_dir.exists():
            print(f"âŒ åˆ†æ®µç›®å½•ä¸å­˜åœ¨: {segments_dir}")
            return None
        
        segments_info_path = segments_dir / "segments.json"
        if not segments_info_path.exists():
            print(f"âŒ åˆ†æ®µä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {segments_info_path}")
            return None
        
        # è¯»å–åˆ†æ®µä¿¡æ¯
        try:
            with open(segments_info_path, 'r', encoding='utf-8') as f:
                segments = json.load(f)
            print(f"ğŸ“‹ åŠ è½½åˆ†æ®µä¿¡æ¯: {len(segments)} ä¸ªç‰‡æ®µ")
        except Exception as e:
            print(f"âŒ è¯»å–åˆ†æ®µä¿¡æ¯å¤±è´¥: {e}")
            return None
        
        # éªŒè¯åˆ†æ®µæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        valid_segments = []
        missing_files = []
        
        for segment in segments:
            segment_path = segments_dir / segment['file']
            if segment_path.exists():
                valid_segments.append(segment)
            else:
                missing_files.append(segment['file'])
        
        if missing_files:
            print(f"âš ï¸ è­¦å‘Š: {len(missing_files)} ä¸ªåˆ†æ®µæ–‡ä»¶ä¸å­˜åœ¨")
            if len(missing_files) <= 5:
                for f in missing_files:
                    print(f"  - {f}")
            else:
                for f in missing_files[:3]:
                    print(f"  - {f}")
                print(f"  ... è¿˜æœ‰ {len(missing_files) - 3} ä¸ªæ–‡ä»¶")
        
        if not valid_segments:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„åˆ†æ®µæ–‡ä»¶")
            return None
        
        print(f"âœ… æ‰¾åˆ° {len(valid_segments)} ä¸ªæœ‰æ•ˆåˆ†æ®µæ–‡ä»¶")
        segments = valid_segments
        
        # ä½¿ç”¨æ™ºèƒ½æ¨¡å‹åŠ è½½å™¨
        self.model = self.smart_loader.load_model(self.model_type, model_dir)
        if not self.model:
            print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
            return None
        
        # ä¼˜åŒ–æ¨¡å‹ä»¥è¿›è¡Œæ¨ç†
        self.smart_loader.optimize_for_inference()
        
        # è·å–æ™ºèƒ½è§£ç é…ç½®
        decode_config = self.smart_loader.get_transcribe_config()
        print(f"ğŸ¯ è§£ç é…ç½®: {decode_config}")
        
        # è·å–å¹¶è¡Œå¤„ç†é…ç½®
        strategy = self.hardware_manager.get_optimal_config()['strategy']
        
        # æ™ºèƒ½é€‰æ‹©å¹¶è¡Œå¤„ç†ç­–ç•¥
        segment_count = len(segments)
        
        if self.model_type == "llm":
            # LLM æ¨¡å‹æ™ºèƒ½å¤„ç†ç­–ç•¥
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†GPUè¾…åŠ©ï¼ˆç¼–ç å™¨åœ¨GPUï¼‰
            gpu_assisted = self.smart_loader.strategy.get('gpu_role') in ['encoder_only', 'feature_extraction']
            
            if gpu_assisted and segment_count > 10:
                # GPUè¾…åŠ©æ¨¡å¼ä¸‹ï¼Œå¯ä»¥ä½¿ç”¨æœ‰é™çš„å¹¶è¡Œ
                max_workers = min(2, max(1, strategy['cpu_threads'] // 8))  # ä¿å®ˆçš„å¹¶è¡Œåº¦
                batch_size = 1
                print(f"ğŸš€ LLM GPUè¾…åŠ©æ¨¡å¼: {segment_count} ä¸ªåˆ†æ®µï¼Œä½¿ç”¨ {max_workers} çº¿ç¨‹å¹¶è¡Œ")
                print("ğŸ“Œ æç¤º: ç¼–ç å™¨åœ¨GPUä¸Šï¼ŒLLMä¸»ä½“åœ¨CPUä¸Šï¼Œé‡‡ç”¨ä¿å®ˆå¹¶è¡Œç­–ç•¥")
            else:
                # çº¯CPUæ¨¡å¼æˆ–å°‘é‡åˆ†æ®µï¼Œä½¿ç”¨ä¸²è¡Œå¤„ç†
                max_workers = 1
                batch_size = 1
                if segment_count <= 10:
                    print(f"âš ï¸ LLM ä¸²è¡Œå¤„ç†: åˆ†æ®µæ•°è¾ƒå°‘({segment_count}ä¸ª)ï¼Œä½¿ç”¨ä¸²è¡Œå¤„ç†")
                else:
                    print("âš ï¸ LLM çº¯CPUæ¨¡å¼ï¼Œä½¿ç”¨ä¸²è¡Œå¤„ç†ä»¥ç¡®ä¿ç¨³å®šæ€§")
        else:
            # AED æ¨¡å‹å¯ä»¥å®‰å…¨åœ°å¹¶è¡Œå¤„ç†
            # æ ¹æ®åˆ†æ®µæ•°é‡å’Œç¡¬ä»¶èƒ½åŠ›æ™ºèƒ½è°ƒæ•´å¹¶è¡Œåº¦
            if segment_count <= 10:
                max_workers = min(2, strategy['cpu_threads'])  # å°‘é‡åˆ†æ®µç”¨å°‘çº¿ç¨‹
            elif segment_count <= 50:
                max_workers = min(4, strategy['cpu_threads'])  # ä¸­ç­‰æ•°é‡åˆ†æ®µ
            else:
                max_workers = min(8, strategy['cpu_threads'])  # å¤§é‡åˆ†æ®µç”¨æ›´å¤šçº¿ç¨‹
            
            batch_size = min(strategy.get('batch_size', 2), 2)  # é™åˆ¶æ‰¹æ¬¡å¤§å°é¿å…å†…å­˜é—®é¢˜
            print(f"ğŸ”§ AED æ™ºèƒ½å¹¶è¡Œ: {segment_count} ä¸ªåˆ†æ®µï¼Œä½¿ç”¨ {max_workers} çº¿ç¨‹")
        
        print(f"ğŸ”§ å¤„ç†é…ç½®: {max_workers} çº¿ç¨‹, æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # å‡†å¤‡éŸ³é¢‘ç‰‡æ®µè·¯å¾„
        segment_paths = [segments_dir / segment['file'] for segment in segments]
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        transcripts_dir = segments_dir.parent / "transcripts"
        transcripts_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºçº¿ç¨‹é”ä»¥ä¿æŠ¤æ¨¡å‹è®¿é—®
        import threading
        model_lock = threading.Lock()
        
        # åˆ›å»ºè½¬å½•å‡½æ•°
        def transcribe_single_segment(segment_path):
            """è½¬å½•å•ä¸ªéŸ³é¢‘ç‰‡æ®µ"""
            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not segment_path.exists():
                    print(f"âš ï¸ è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶: {segment_path.name}")
                    return None
                
                # æ‰¾åˆ°å¯¹åº”çš„ segment ä¿¡æ¯
                segment_info = None
                for seg in segments:
                    if segments_dir / seg['file'] == segment_path:
                        segment_info = seg
                        break
                
                if not segment_info:
                    print(f"âš ï¸ æ‰¾ä¸åˆ°åˆ†æ®µä¿¡æ¯: {segment_path.name}")
                    return None
                
                uttid = f"segment_{segment_info['index']:03d}"
                
                # ä½¿ç”¨é”ä¿æŠ¤æ¨¡å‹è°ƒç”¨
                start_time = time.time()
                with model_lock:
                    # æ¸…ç†ç¼“å­˜
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
                    # è°ƒç”¨æ¨¡å‹
                    result = self.model.transcribe([uttid], [str(segment_path)], decode_config)
                
                process_time = time.time() - start_time
                
                if result and len(result) > 0:
                    text = result[0]['text']
                    rtf = float(result[0].get('rtf', 0))
                    
                    # ä¿å­˜å•ä¸ªç»“æœ
                    transcript_path = transcripts_dir / f"{uttid}.txt"
                    with open(transcript_path, 'w', encoding='utf-8') as f:
                        f.write(text)
                    
                    return {
                        'index': segment_info['index'],
                        'file': segment_info['file'],
                        'start': segment_info['start'],
                        'end': segment_info['end'],
                        'duration': segment_info['duration'],
                        'text': text,
                        'rtf': rtf,
                        'process_time': process_time
                    }
                else:
                    print(f"âš ï¸ æ¨¡å‹è½¬å½•æ— ç»“æœ: {segment_path.name}")
                    return None
                
            except Exception as e:
                print(f"âŒ è½¬å½•ç‰‡æ®µå¤±è´¥ {segment_path.name}: {str(e)}")
                import traceback
                traceback.print_exc()
                return None
        
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å¤„ç†æ–¹å¼
        if max_workers == 1:
            # ä¸²è¡Œå¤„ç†
            print(f"ğŸš€ ä¸²è¡Œè½¬å†™ {len(segment_paths)} ä¸ªç‰‡æ®µ...")
            results = []
            for i, segment_path in enumerate(segment_paths):
                print(f"å¤„ç†ç‰‡æ®µ {i+1}/{len(segment_paths)}: {segment_path.name}")
                result = transcribe_single_segment(segment_path)
                if result:
                    results.append(result)
                # å®šæœŸæ¸…ç†å†…å­˜
                if (i + 1) % 10 == 0:
                    import gc
                    gc.collect()
        else:
            # å¹¶è¡Œå¤„ç†
            print(f"ğŸš€ ä½¿ç”¨ {max_workers} çº¿ç¨‹å¹¶è¡Œè½¬å†™ {len(segment_paths)} ä¸ªç‰‡æ®µ...")
            processor = AudioBatchProcessor(max_workers=max_workers)
            results = processor.process_audio_segments(
                segment_paths, 
                transcribe_single_segment,
                batch_size=batch_size,
                model_type=self.model_type
            )
        
        # ä¿å­˜è½¬å†™ç»“æœæ±‡æ€»
        if results:
            try:
                results_path = segments_dir.parent / "transcripts.json"
                results_path.parent.mkdir(parents=True, exist_ok=True)
                with open(results_path, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                
                total = len(segments)
                print(f"\nâœ… æ™ºèƒ½æ‰¹é‡è½¬å†™å®Œæˆ: {len(results)}/{total} æˆåŠŸ")
                return results
            except Exception as e:
                print(f"âŒ ä¿å­˜è½¬å†™ç»“æœå¤±è´¥: {e}")
                return results  # è¿”å›ç»“æœä½†è®°å½•ä¿å­˜å¤±è´¥
        else:
            print("\nâŒ æ²¡æœ‰æˆåŠŸè½¬å†™çš„ç‰‡æ®µ")
            return None
    
    def generate_unique_filename(self, base_path, extension):
        """ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼Œé¿å…è¦†ç›–"""
        counter = 1
        final_path = Path(f"{base_path}{extension}")
        
        while final_path.exists():
            counter += 1
            final_path = Path(f"{base_path}_{counter}{extension}")
        
        return final_path
    
    def concatenate_results(self, results, input_filename):
        """æ‹¼æ¥è½¬å†™ç»“æœ"""
        print("\nğŸ“ æ‹¼æ¥è½¬å†™ç»“æœ...")
        
        # æŒ‰æ—¶é—´æ’åº
        results.sort(key=lambda x: x['start'])
        
        # è·å–è¾“å…¥æ–‡ä»¶çš„åŸºæœ¬åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰
        base_name = Path(input_filename).stem
        output_base = self.output_dir / base_name
        
        # ç”Ÿæˆçº¯æ–‡æœ¬
        full_text = []
        for result in results:
            full_text.append(result['text'])
        
        txt_path = self.generate_unique_filename(output_base, ".txt")
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(full_text))
        print(f"âœ… ç”Ÿæˆçº¯æ–‡æœ¬: {txt_path.name}")
        
        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡æœ¬
        timestamp_text = []
        for result in results:
            start_time = str(timedelta(seconds=result['start'])).split('.')[0]
            end_time = str(timedelta(seconds=result['end'])).split('.')[0]
            timestamp_text.append(f"[{start_time} --> {end_time}]")
            timestamp_text.append(result['text'])
            timestamp_text.append("")
        
        timestamp_path = self.generate_unique_filename(output_base, "_æ—¶é—´æˆ³.txt")
        with open(timestamp_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(timestamp_text))
        print(f"âœ… ç”Ÿæˆæ—¶é—´æˆ³æ–‡æœ¬: {timestamp_path.name}")
        
        # ç”Ÿæˆ SRT å­—å¹•
        srt_lines = []
        for i, result in enumerate(results, 1):
            start_time = self.seconds_to_srt_time(result['start'])
            end_time = self.seconds_to_srt_time(result['end'])
            srt_lines.append(str(i))
            srt_lines.append(f"{start_time} --> {end_time}")
            srt_lines.append(result['text'])
            srt_lines.append("")
        
        srt_path = self.generate_unique_filename(output_base, ".srt")
        with open(srt_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(srt_lines))
        print(f"âœ… ç”Ÿæˆå­—å¹•æ–‡ä»¶: {srt_path.name}")
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        total_duration = results[-1]['end'] if results else 0
        total_process_time = sum(r.get('process_time', 0) for r in results)
        avg_rtf = sum(r.get('rtf', 0) for r in results) / len(results) if results else 0
        
        stats = {
            'total_segments': len(results),
            'total_duration': total_duration,
            'total_duration_formatted': str(timedelta(seconds=total_duration)),
            'total_process_time': total_process_time,
            'average_rtf': avg_rtf,
            'total_characters': sum(len(r['text']) for r in results),
            'model_type': self.model_type
        }
        
        stats_path = self.generate_unique_filename(output_base, "_ç»Ÿè®¡.json")
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ€»æ—¶é•¿: {stats['total_duration_formatted']}")
        print(f"   å¤„ç†æ—¶é—´: {total_process_time:.2f}s")
        print(f"   å¹³å‡ RTF: {avg_rtf:.4f}")
        print(f"   æ€»å­—ç¬¦æ•°: {stats['total_characters']}")
        
        # æ ‡ç‚¹æ¢å¤å¤„ç†
        if self.enable_punctuation:
            try:
                print(f"\nğŸ”¤ å¼€å§‹æ ‡ç‚¹æ¢å¤å¤„ç†...")
                
                # åˆå§‹åŒ–æ ‡ç‚¹æ¢å¤å™¨ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
                if self.punctuation_restorer is None:
                    self.punctuation_restorer = PunctuationRestorer(
                        cache_dir=self.punctuation_model_dir,
                        chunk_size=self.punctuation_chunk_size,
                        stride=self.punctuation_stride
                    )
                    
                # å¯¹çº¯æ–‡æœ¬è¿›è¡Œæ ‡ç‚¹æ¢å¤
                full_text_content = '\n'.join(full_text)
                punctuated_text = self.punctuation_restorer.restore_punctuation(full_text_content)
                
                # ä¿å­˜å¸¦æ ‡ç‚¹çš„çº¯æ–‡æœ¬
                punctuated_txt_path = self.generate_unique_filename(output_base, "_æ ‡ç‚¹.txt")
                with open(punctuated_txt_path, 'w', encoding='utf-8') as f:
                    f.write(punctuated_text)
                print(f"âœ… ç”Ÿæˆå¸¦æ ‡ç‚¹æ–‡æœ¬: {punctuated_txt_path.name}")
                
                # ç”Ÿæˆå¸¦æ ‡ç‚¹çš„ SRT å­—å¹•
                # å°†å¸¦æ ‡ç‚¹çš„æ–‡æœ¬æŒ‰åŸå§‹åˆ†æ®µé‡æ–°åˆ†é…
                punctuated_lines = punctuated_text.split('\n')
                if len(punctuated_lines) == len(results):
                    # å¦‚æœè¡Œæ•°åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨
                    for i, result in enumerate(results):
                        if i < len(punctuated_lines):
                            result['punctuated_text'] = punctuated_lines[i]
                        else:
                            result['punctuated_text'] = result['text']
                else:
                    # å¦‚æœè¡Œæ•°ä¸åŒ¹é…ï¼Œå°è¯•æŒ‰å­—ç¬¦é•¿åº¦åˆ†é…
                    punctuated_full = punctuated_text.replace('\n', ' ')
                    char_offset = 0
                    for result in results:
                        orig_len = len(result['text'])
                        result['punctuated_text'] = punctuated_full[char_offset:char_offset + orig_len].strip()
                        char_offset += orig_len
                
                # ç”Ÿæˆå¸¦æ ‡ç‚¹çš„ SRT
                punctuated_srt_lines = []
                for i, result in enumerate(results, 1):
                    start_time = self.seconds_to_srt_time(result['start'])
                    end_time = self.seconds_to_srt_time(result['end'])
                    punctuated_srt_lines.append(str(i))
                    punctuated_srt_lines.append(f"{start_time} --> {end_time}")
                    punctuated_srt_lines.append(result.get('punctuated_text', result['text']))
                    punctuated_srt_lines.append("")
                
                punctuated_srt_path = self.generate_unique_filename(output_base, "_æ ‡ç‚¹.srt")
                with open(punctuated_srt_path, 'w', encoding='utf-8') as f:
                    f.write('\n'.join(punctuated_srt_lines))
                print(f"âœ… ç”Ÿæˆå¸¦æ ‡ç‚¹å­—å¹•: {punctuated_srt_path.name}")
                
                # å¦‚æœå¯ç”¨äº†åˆ†æ®µåŠŸèƒ½
                if self.enable_paragraph and punctuated_text:
                    try:
                        print(f"\nğŸ“‘ å¼€å§‹è‡ªç„¶æ®µåˆ†æ®µå¤„ç†...")
                        
                        # åˆå§‹åŒ–åˆ†æ®µå™¨
                        if self.paragraph_segmenter is None:
                            self.paragraph_segmenter = ParagraphSegmenter(
                                min_length=self.min_paragraph_length,
                                max_length=self.max_paragraph_length
                            )
                        
                        # æ‰§è¡Œåˆ†æ®µ
                        paragraphs = self.paragraph_segmenter.segment_paragraphs(punctuated_text)
                        
                        # ä¿å­˜åˆ†æ®µç»“æœ
                        paragraph_txt_path = self.generate_unique_filename(output_base, "_æ®µè½.txt")
                        with open(paragraph_txt_path, 'w', encoding='utf-8') as f:
                            f.write(f"FireRedASR è§†é¢‘è½¬å†™ç»“æœï¼ˆè‡ªç„¶æ®µæ ¼å¼ï¼‰\n")
                            f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                            f.write(f"æ®µè½æ•°: {len(paragraphs)}\n")
                            f.write("=" * 60 + "\n\n")
                            
                            for i, para in enumerate(paragraphs, 1):
                                f.write(f"ã€ç¬¬{i}æ®µã€‘\n{para}\n\n")
                        
                        print(f"âœ… ç”Ÿæˆè‡ªç„¶æ®µæ–‡ä»¶: {paragraph_txt_path.name}")
                        print(f"   å…±åˆ†ä¸º {len(paragraphs)} ä¸ªè‡ªç„¶æ®µ")
                        
                    except Exception as e:
                        print(f"âš ï¸ åˆ†æ®µå¤„ç†å¤±è´¥: {str(e)}")
                        print("   å°†ä¿ç•™å¸¦æ ‡ç‚¹ç‰ˆæœ¬")
                
            except Exception as e:
                print(f"âš ï¸ æ ‡ç‚¹æ¢å¤å¤±è´¥: {str(e)}")
                print("   å°†ä¿ç•™æ— æ ‡ç‚¹ç‰ˆæœ¬")
    
    def seconds_to_srt_time(self, seconds):
        """å°†ç§’æ•°è½¬æ¢ä¸º SRT æ—¶é—´æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = seconds % 60
        return f"{hours:02d}:{minutes:02d}:{secs:06.3f}".replace('.', ',')
    
    def process_long_video(self, input_path):
        """å¤„ç†å•ä¸ªé•¿è§†é¢‘æ–‡ä»¶çš„å®Œæ•´æµç¨‹"""
        print(f"\n{'='*60}")
        print(f"ğŸ¬ å¤„ç†æ–‡ä»¶: {input_path.name}")
        print(f"{'='*60}")
        
        # åˆ›å»ºä¸´æ—¶å·¥ä½œç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        work_dir = self.temp_dir / f"{input_path.stem}_{timestamp}"
        work_dir.mkdir(parents=True, exist_ok=True)
        
        segments_dir = work_dir / "segments"
        segments_dir.mkdir(exist_ok=True)
        
        try:
            # æ­¥éª¤1ï¼šå‡†å¤‡éŸ³é¢‘
            print("\n[æ­¥éª¤ 1/4] å‡†å¤‡éŸ³é¢‘...")
            prepared_audio = work_dir / "prepared_audio.wav"
            if not self.prepare_audio(input_path, prepared_audio):
                return False
            
            # æ­¥éª¤2ï¼šVAD åˆ‡ç‰‡
            print("\n[æ­¥éª¤ 2/4] VAD è¯­éŸ³æ£€æµ‹å’Œåˆ‡ç‰‡...")
            segments = self.slice_audio_with_vad(prepared_audio, segments_dir)
            if not segments:
                return False
            
            # æ­¥éª¤3ï¼šæ‰¹é‡è½¬å†™
            print("\n[æ­¥éª¤ 3/4] æ‰¹é‡è½¬å†™...")
            model_dir = self.get_model_dir()
            if not model_dir:
                return False
            
            results = self.batch_transcribe(segments_dir, model_dir)
            if not results:
                return False
            
            # æ­¥éª¤4ï¼šæ‹¼æ¥ç»“æœ
            print("\n[æ­¥éª¤ 4/4] æ‹¼æ¥ç»“æœ...")
            self.concatenate_results(results, input_path.name)
            
            print(f"\nâœ… å¤„ç†å®Œæˆï¼")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            # shutil.rmtree(work_dir)
            
            return True
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
        finally:
            # æ¸…ç†æ¨¡å‹
            if self.model:
                self.model.feat_extractor.cleanup_temp_files()
    
    def run(self):
        """è¿è¡Œé•¿è§†é¢‘æ‰¹é‡å¤„ç†"""
        print("ğŸ”¥ FireRedASR é•¿è§†é¢‘è½¬æ–‡å­—æ‰¹é‡å¤„ç†ç³»ç»Ÿ")
        print("=" * 60)
        
        # æ£€æŸ¥ä¾èµ–
        if not self.check_dependencies():
            return
        
        # æ‰«ææ–‡ä»¶
        files = self.scan_long_media_files()
        if not self.display_files(files):
            return
        
        # ç”¨æˆ·ç¡®è®¤
        try:
            confirm = input(f"\næ˜¯å¦å¤„ç†è¿™ {len(files)} ä¸ªé•¿è§†é¢‘æ–‡ä»¶? (y/n): ").strip().lower()
            if confirm not in ['y', 'yes', 'æ˜¯']:
                print("ğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                return
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return
        
        # è¯¢é—® VAD å‚æ•°
        try:
            custom = input("\næ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰ VAD å‚æ•°? (y/n) [é»˜è®¤: n]: ").strip().lower()
            if custom in ['y', 'yes', 'æ˜¯']:
                self.max_speech_duration_s = int(input("æœ€å¤§è¯­éŸ³æ®µé•¿åº¦ï¼ˆç§’ï¼‰[é»˜è®¤: 30]: ") or "30")
                self.min_silence_duration_ms = int(input("æœ€å°é™éŸ³é—´éš”ï¼ˆæ¯«ç§’ï¼‰[é»˜è®¤: 500]: ") or "500")
                self.min_speech_duration_ms = int(input("æœ€å°è¯­éŸ³æ®µé•¿åº¦ï¼ˆæ¯«ç§’ï¼‰[é»˜è®¤: 1000]: ") or "1000")
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return
        
        # æ‰¹é‡å¤„ç†
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†...")
        success_count = 0
        
        for i, file_path in enumerate(files, 1):
            print(f"\n\n[{i}/{len(files)}] å¤„ç†è¿›åº¦")
            if self.process_long_video(file_path):
                success_count += 1
            
            # è¯¢é—®æ˜¯å¦ç»§ç»­
            if i < len(files):
                try:
                    cont = input("\nç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶? (y/n) [é»˜è®¤: y]: ").strip().lower()
                    if cont in ['n', 'no', 'å¦']:
                        print("ğŸ‘‹ ç”¨æˆ·åœæ­¢å¤„ç†")
                        break
                except KeyboardInterrupt:
                    print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­å¤„ç†")
                    break
        
        # æ€»ç»“
        print("\n" + "=" * 60)
        print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š æ€»è®¡: {len(files)} ä¸ªæ–‡ä»¶, æˆåŠŸ: {success_count} ä¸ª")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {self.output_dir}")
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if self.temp_dir.exists() and not any(self.temp_dir.iterdir()):
            self.temp_dir.rmdir()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='FireRedASR é•¿è§†é¢‘è½¬æ–‡å­—æ‰¹é‡å¤„ç†ç³»ç»Ÿ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s                                # äº¤äº’å¼å¤„ç†
  %(prog)s --model_type llm               # ä½¿ç”¨ LLM æ¨¡å‹
  %(prog)s --max_duration 45              # è®¾ç½®æœ€å¤§æ®µé•¿ä¸º 45 ç§’
  %(prog)s --min_silence 300              # è®¾ç½®æœ€å°é™éŸ³ä¸º 300 æ¯«ç§’
        """
    )
    
    parser.add_argument('--model_type', type=str, choices=['aed', 'llm'],
                        help='æ¨¡å‹ç±»å‹ï¼ˆå¦‚ä¸æŒ‡å®šåˆ™äº¤äº’å¼é€‰æ‹©ï¼‰')
    parser.add_argument('--max_duration', type=int, default=30,
                        help='æœ€å¤§è¯­éŸ³æ®µé•¿åº¦ï¼ˆç§’ï¼‰')
    parser.add_argument('--min_silence', type=int, default=500,
                        help='æœ€å°é™éŸ³é—´éš”ï¼ˆæ¯«ç§’ï¼‰')
    parser.add_argument('--min_speech', type=int, default=1000,
                        help='æœ€å°è¯­éŸ³æ®µé•¿åº¦ï¼ˆæ¯«ç§’ï¼‰')
    
    # æ ‡ç‚¹æ¢å¤ç›¸å…³å‚æ•°
    parser.add_argument('--enable-punctuation', action='store_true', default=True,
                        help='å¯ç”¨æ ‡ç‚¹æ¢å¤ï¼ˆé»˜è®¤å¯ç”¨ï¼‰')
    parser.add_argument('--disable-punctuation', action='store_true',
                        help='ç¦ç”¨æ ‡ç‚¹æ¢å¤')
    parser.add_argument('--punctuation-model-dir', type=str,
                        help='è‡ªå®šä¹‰æ ‡ç‚¹æ¢å¤æ¨¡å‹è·¯å¾„')
    parser.add_argument('--punctuation-chunk-size', type=int, default=256,
                        help='æ ‡ç‚¹æ¢å¤æ–‡æœ¬å—å¤§å°ï¼ˆé»˜è®¤: 256ï¼‰')
    parser.add_argument('--punctuation-stride', type=int, default=128,
                        help='æ ‡ç‚¹æ¢å¤æ»‘åŠ¨çª—å£æ­¥é•¿ï¼ˆé»˜è®¤: 128ï¼‰')
    
    # åˆ†æ®µç›¸å…³å‚æ•°
    parser.add_argument('--enable-paragraph', action='store_true',
                        help='å¯ç”¨è‡ªç„¶æ®µåˆ†æ®µåŠŸèƒ½')
    parser.add_argument('--paragraph-method', type=str, default='rule',
                        choices=['rule', 'semantic', 'hybrid'],
                        help='åˆ†æ®µæ–¹æ³•ï¼šruleï¼ˆè§„åˆ™ï¼‰ã€semanticï¼ˆè¯­ä¹‰ï¼‰ã€hybridï¼ˆæ··åˆï¼‰')
    parser.add_argument('--min-paragraph-length', type=int, default=50,
                        help='æœ€å°æ®µè½é•¿åº¦ï¼ˆé»˜è®¤: 50å­—ï¼‰')
    parser.add_argument('--max-paragraph-length', type=int, default=500,
                        help='æœ€å¤§æ®µè½é•¿åº¦ï¼ˆé»˜è®¤: 500å­—ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ˜¯å¦åœ¨æ­£ç¡®çš„ç›®å½•
    if not Path("fireredasr").exists():
        print("âŒ é”™è¯¯: è¯·åœ¨ FireRedASR é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
        return
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['PYTHONPATH'] = str(Path.cwd()) + ":" + os.environ.get('PYTHONPATH', '')
    
    try:
        transcriber = LongVideoTranscriber()
        
        # è®¾ç½®å‚æ•°
        if args.model_type:
            transcriber.model_type = args.model_type
        transcriber.max_speech_duration_s = args.max_duration
        transcriber.min_silence_duration_ms = args.min_silence
        transcriber.min_speech_duration_ms = args.min_speech
        
        # è®¾ç½®æ ‡ç‚¹æ¢å¤å‚æ•°
        if args.disable_punctuation:
            transcriber.enable_punctuation = False
        else:
            transcriber.enable_punctuation = True
        
        if args.punctuation_model_dir:
            transcriber.punctuation_model_dir = args.punctuation_model_dir
        transcriber.punctuation_chunk_size = args.punctuation_chunk_size
        transcriber.punctuation_stride = args.punctuation_stride
        
        # è®¾ç½®åˆ†æ®µå‚æ•°
        transcriber.enable_paragraph = args.enable_paragraph
        transcriber.paragraph_method = args.paragraph_method
        transcriber.min_paragraph_length = args.min_paragraph_length
        transcriber.max_paragraph_length = args.max_paragraph_length
        
        transcriber.run()
        
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()