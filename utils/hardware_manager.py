#!/usr/bin/env python3
"""
ç¡¬ä»¶èµ„æºç®¡ç†æ¨¡å—
- GPU æ˜¾å­˜æ£€æµ‹
- CPU æ ¸å¿ƒæ•°æ£€æµ‹
- æ™ºèƒ½èµ„æºåˆ†é…ç­–ç•¥
"""

import os
import psutil
import torch
from typing import Dict, Tuple


class HardwareManager:
    """ç¡¬ä»¶èµ„æºç®¡ç†å™¨"""
    
    def __init__(self):
        self.cpu_count = psutil.cpu_count()
        self.memory_gb = psutil.virtual_memory().total / (1024**3)
        self.gpu_info = self._detect_gpu()
        self.strategy = self._select_strategy()
    
    def _detect_gpu(self) -> Dict:
        """æ£€æµ‹ GPU ä¿¡æ¯"""
        gpu_info = {
            'available': False,
            'memory_gb': 0,
            'device_name': 'None'
        }
        
        if torch.cuda.is_available():
            try:
                device = torch.cuda.current_device()
                gpu_info['available'] = True
                gpu_info['memory_gb'] = torch.cuda.get_device_properties(device).total_memory / (1024**3)
                gpu_info['device_name'] = torch.cuda.get_device_name(device)
                
                # è·å–å½“å‰å¯ç”¨æ˜¾å­˜ï¼ˆå‡å»å·²å ç”¨çš„ï¼‰
                torch.cuda.empty_cache()
                available_memory = torch.cuda.get_device_properties(device).total_memory
                allocated_memory = torch.cuda.memory_allocated(device)
                gpu_info['available_gb'] = (available_memory - allocated_memory) / (1024**3)
                
            except Exception as e:
                print(f"âš ï¸ GPU æ£€æµ‹å¤±è´¥: {e}")
                
        return gpu_info
    
    def _select_strategy(self) -> Dict:
        """æ™ºèƒ½é€‰æ‹©å¤„ç†ç­–ç•¥"""
        # é»˜è®¤ç­–ç•¥ï¼šCPUä¸ºä¸»ï¼Œæ¸©å’Œä½¿ç”¨èµ„æº
        strategy = {
            'name': 'cpu_primary',
            'use_gpu': False,
            'cpu_threads': min(16, max(8, self.cpu_count - 2)),  # ä½¿ç”¨æ›´å¤šCPUæ ¸å¿ƒï¼Œä½†ä¿ç•™2æ ¸ç»™ç³»ç»Ÿ
            'batch_size': 1,
            'gpu_batch_size': 1,
            'memory_management': 'balanced'
        }
        
        if self.gpu_info['available']:
            gpu_memory = self.gpu_info.get('available_gb', 0)
            
            # å³ä½¿æœ‰GPUï¼Œä¹Ÿä¼˜å…ˆä½¿ç”¨CPUï¼ŒGPUåªä½œä¸ºè¾…åŠ©
            if gpu_memory >= 8:
                # å¤§æ˜¾å­˜ï¼šCPUä¸ºä¸»ï¼ŒGPUåŠ é€Ÿå…³é”®éƒ¨åˆ†
                strategy.update({
                    'name': 'cpu_primary_gpu_assist',
                    'use_gpu': True,
                    'cpu_threads': min(16, max(12, self.cpu_count - 2)),
                    'batch_size': 2,
                    'gpu_batch_size': 1,
                    'memory_management': 'cpu_optimized',
                    'gpu_role': 'encoder_only'  # GPUä»…ç”¨äºç¼–ç å™¨
                })
            elif gpu_memory >= 4:
                # ä¸­ç­‰æ˜¾å­˜ï¼šCPUå¤„ç†ä¸»ä½“ï¼ŒGPUå¤„ç†å°éƒ¨åˆ†
                strategy.update({
                    'name': 'cpu_primary_minimal_gpu',
                    'use_gpu': True,
                    'cpu_threads': min(16, max(10, self.cpu_count - 2)),
                    'batch_size': 1,
                    'gpu_batch_size': 1,
                    'memory_management': 'cpu_focused',
                    'gpu_role': 'feature_extraction'  # GPUä»…ç”¨äºç‰¹å¾æå–
                })
            elif gpu_memory >= 2:
                # å°æ˜¾å­˜ï¼šä¸»è¦ä¾èµ–CPU
                strategy.update({
                    'name': 'cpu_dominant',
                    'use_gpu': False,  # å°æ˜¾å­˜æ—¶ä¸ä½¿ç”¨GPUï¼Œé¿å…OOM
                    'cpu_threads': min(16, max(8, self.cpu_count - 2)),
                    'batch_size': 1,
                    'gpu_batch_size': 0,
                    'memory_management': 'cpu_only'
                })
        
        return strategy
    
    def get_optimal_config(self) -> Dict:
        """è·å–ä¼˜åŒ–é…ç½®"""
        return {
            'hardware': {
                'cpu_cores': self.cpu_count,
                'memory_gb': self.memory_gb,
                'gpu_available': self.gpu_info['available'],
                'gpu_memory_gb': self.gpu_info.get('memory_gb', 0),
                'gpu_available_gb': self.gpu_info.get('available_gb', 0),
                'gpu_name': self.gpu_info.get('device_name', 'None')
            },
            'strategy': self.strategy,
            'limits': {
                'max_cpu_usage': 80,  # æœ€å¤§ CPU ä½¿ç”¨ç‡
                'max_memory_usage': 85,  # æœ€å¤§å†…å­˜ä½¿ç”¨ç‡
                'max_gpu_memory_usage': 90  # æœ€å¤§æ˜¾å­˜ä½¿ç”¨ç‡
            }
        }
    
    def monitor_resources(self) -> Dict:
        """ç›‘æ§å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        
        status = {
            'cpu_usage': cpu_percent,
            'memory_usage': memory.percent,
            'memory_available_gb': memory.available / (1024**3)
        }
        
        if self.gpu_info['available']:
            try:
                gpu_memory_used = torch.cuda.memory_allocated() / (1024**3)
                gpu_memory_total = self.gpu_info['memory_gb']
                status['gpu_memory_usage'] = (gpu_memory_used / gpu_memory_total) * 100
                status['gpu_memory_used_gb'] = gpu_memory_used
            except:
                status['gpu_memory_usage'] = 0
                status['gpu_memory_used_gb'] = 0
        
        return status
    
    def should_reduce_load(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦é™ä½è´Ÿè½½"""
        status = self.monitor_resources()
        
        # CPU ä½¿ç”¨ç‡è¿‡é«˜
        if status['cpu_usage'] > 85:
            return True
        
        # å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜
        if status['memory_usage'] > 90:
            return True
        
        # GPU æ˜¾å­˜ä½¿ç”¨ç‡è¿‡é«˜
        if self.gpu_info['available'] and status.get('gpu_memory_usage', 0) > 95:
            return True
        
        return False
    
    def print_hardware_info(self):
        """æ‰“å°ç¡¬ä»¶ä¿¡æ¯"""
        print("ğŸ–¥ï¸ ç¡¬ä»¶é…ç½®æ£€æµ‹:")
        print(f"   CPU: {self.cpu_count} æ ¸å¿ƒ")
        print(f"   å†…å­˜: {self.memory_gb:.1f} GB")
        
        if self.gpu_info['available']:
            print(f"   GPU: {self.gpu_info['device_name']}")
            print(f"   æ˜¾å­˜: {self.gpu_info['memory_gb']:.1f} GB (å¯ç”¨: {self.gpu_info.get('available_gb', 0):.1f} GB)")
        else:
            print("   GPU: æœªæ£€æµ‹åˆ°æˆ–ä¸å¯ç”¨")
        
        print(f"\nâš™ï¸ é€‰æ‹©ç­–ç•¥: {self.strategy['name']}")
        print(f"   CPU çº¿ç¨‹: {self.strategy['cpu_threads']}")
        print(f"   æ‰¹å¤„ç†å¤§å°: {self.strategy['batch_size']}")
        print(f"   ä½¿ç”¨ GPU: {'æ˜¯' if self.strategy['use_gpu'] else 'å¦'}")
        print(f"   å†…å­˜ç®¡ç†: {self.strategy['memory_management']}")


def get_hardware_manager() -> HardwareManager:
    """è·å–ç¡¬ä»¶ç®¡ç†å™¨å®ä¾‹"""
    return HardwareManager()


if __name__ == "__main__":
    # æµ‹è¯•ç¡¬ä»¶æ£€æµ‹
    manager = HardwareManager()
    manager.print_hardware_info()
    
    print("\nğŸ“Š å½“å‰èµ„æºä½¿ç”¨:")
    status = manager.monitor_resources()
    for key, value in status.items():
        print(f"   {key}: {value}")