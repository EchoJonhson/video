#!/usr/bin/env python3
"""
æ™ºèƒ½æ¨¡åž‹åŠ è½½å™¨
- æ”¯æŒå°æ˜¾å­˜çš„åŠ¨æ€æ¨¡åž‹åŠ è½½
- å†…å­˜æ˜ å°„å’Œåˆ†å±‚åŠ è½½
- GPU/CPU æ··åˆæ¨¡å¼ç®¡ç†
"""

import os
import gc
import time
import torch
from pathlib import Path
from typing import Dict, Optional, Any
from contextlib import contextmanager

from fireredasr.models.fireredasr import FireRedAsr
from .hardware_manager import HardwareManager


class SmartModelLoader:
    """æ™ºèƒ½æ¨¡åž‹åŠ è½½å™¨"""
    
    def __init__(self, hardware_manager: HardwareManager):
        self.hardware_manager = hardware_manager
        self.config = hardware_manager.get_optimal_config()
        self.strategy = self.config['strategy']
        self.model = None
        self.model_parts = {}
        self.current_device = 'cpu'
        
    def load_model(self, model_type: str, model_dir: str) -> Optional[FireRedAsr]:
        """æ™ºèƒ½åŠ è½½æ¨¡åž‹"""
        print(f"ðŸ¤– åŠ è½½ {model_type.upper()} æ¨¡åž‹...")
        print(f"ðŸ“‹ ä½¿ç”¨ç­–ç•¥: {self.strategy['name']}")
        
        start_time = time.time()
        
        try:
            # æ ¹æ®ç­–ç•¥é€‰æ‹©åŠ è½½æ–¹å¼
            if self.strategy['name'] in ['cpu_primary', 'cpu_dominant', 'cpu_only']:
                self.model = self._load_cpu_only(model_type, model_dir)
            elif self.strategy['name'] == 'cpu_primary_gpu_assist':
                self.model = self._load_cpu_primary_gpu_assist(model_type, model_dir)
            elif self.strategy['name'] == 'cpu_primary_minimal_gpu':
                self.model = self._load_cpu_primary_minimal_gpu(model_type, model_dir)
            elif self.strategy['name'] == 'minimal_gpu':
                self.model = self._load_minimal_gpu(model_type, model_dir)
            elif self.strategy['name'] == 'smart_management':
                self.model = self._load_smart_management(model_type, model_dir)
            elif self.strategy['name'] == 'hybrid_large':
                self.model = self._load_hybrid_large(model_type, model_dir)
            else:
                # é»˜è®¤ CPU æ¨¡å¼
                self.model = self._load_cpu_only(model_type, model_dir)
            
            load_time = time.time() - start_time
            print(f"âœ… æ¨¡åž‹åŠ è½½å®Œæˆ (è€—æ—¶: {load_time:.2f}s)")
            
            self._print_memory_usage()
            return self.model
            
        except Exception as e:
            print(f"âŒ æ¨¡åž‹åŠ è½½å¤±è´¥: {str(e)}")
            # å°è¯•é™çº§åˆ° CPU æ¨¡å¼
            print("ðŸ”„ å°è¯•é™çº§åˆ° CPU æ¨¡å¼...")
            try:
                self.model = self._load_cpu_only(model_type, model_dir)
                print("âœ… CPU æ¨¡å¼åŠ è½½æˆåŠŸ")
                return self.model
            except Exception as e2:
                print(f"âŒ CPU æ¨¡å¼ä¹Ÿå¤±è´¥: {str(e2)}")
                return None
    
    def _load_cpu_only(self, model_type: str, model_dir: str) -> FireRedAsr:
        """çº¯ CPU æ¨¡å¼åŠ è½½"""
        print("ðŸ–¥ï¸ çº¯ CPU æ¨¡å¼")
        
        # ç¡®ä¿åœ¨ CPU ä¸ŠåŠ è½½
        with torch.no_grad():
            model = FireRedAsr.from_pretrained(model_type, model_dir)
            model.model.cpu()
            
        self.current_device = 'cpu'
        return model
    
    def _load_minimal_gpu(self, model_type: str, model_dir: str) -> FireRedAsr:
        """æœ€å° GPU ä½¿ç”¨æ¨¡å¼"""
        print("ðŸŽ¯ æœ€å° GPU æ¨¡å¼")
        
        model = FireRedAsr.from_pretrained(model_type, model_dir)
        
        if model_type == "llm":
            # ä»…ç¼–ç å™¨ç”¨ GPUï¼ŒLLM ä¿æŒ CPU
            if hasattr(model.model, 'encoder'):
                model.model.encoder.cuda()
            if hasattr(model.model, 'encoder_projector'):
                model.model.encoder_projector.cuda()
            # LLM ä¿æŒåœ¨ CPU
            if hasattr(model.model, 'llm'):
                model.model.llm.cpu()
        else:
            # AED æ¨¡å¼å…¨éƒ¨ GPU
            model.model.cuda()
            
        self.current_device = 'cuda'
        return model
    
    def _load_smart_management(self, model_type: str, model_dir: str) -> FireRedAsr:
        """æ™ºèƒ½æ˜¾å­˜ç®¡ç†æ¨¡å¼ï¼ˆæ–¹æ¡ˆ Cï¼‰"""
        print("ðŸ§  æ™ºèƒ½æ˜¾å­˜ç®¡ç†æ¨¡å¼")
        
        model = FireRedAsr.from_pretrained(model_type, model_dir)
        
        # èŽ·å–å½“å‰å¯ç”¨æ˜¾å­˜
        available_memory = self._get_available_gpu_memory()
        print(f"ðŸ’¾ å¯ç”¨æ˜¾å­˜: {available_memory:.2f} GB")
        
        if model_type == "llm":
            # åˆ†å±‚åŠ è½½ç­–ç•¥
            if available_memory >= 6:
                # è¶³å¤Ÿæ˜¾å­˜ï¼šç¼–ç å™¨ + éƒ¨åˆ† LLM å±‚
                print("ðŸ“ˆ é«˜æ˜¾å­˜é…ç½®ï¼šç¼–ç å™¨ + éƒ¨åˆ† LLM å±‚åœ¨ GPU")
                self._smart_llm_placement(model, available_memory)
            elif available_memory >= 3:
                # ä¸­ç­‰æ˜¾å­˜ï¼šä»…ç¼–ç å™¨
                print("ðŸ“Š ä¸­ç­‰æ˜¾å­˜é…ç½®ï¼šä»…ç¼–ç å™¨åœ¨ GPU")
                if hasattr(model.model, 'encoder'):
                    model.model.encoder.cuda()
                if hasattr(model.model, 'encoder_projector'):
                    model.model.encoder_projector.cuda()
                if hasattr(model.model, 'llm'):
                    model.model.llm.cpu()
            else:
                # ä½Žæ˜¾å­˜ï¼šå…¨éƒ¨ CPU
                print("ðŸ“‰ ä½Žæ˜¾å­˜é…ç½®ï¼šå…¨éƒ¨åœ¨ CPU")
                model.model.cpu()
        else:
            # AED æ¨¡å¼
            if available_memory >= 3:
                model.model.cuda()
            else:
                model.model.cpu()
        
        self.current_device = 'cuda' if available_memory >= 1 else 'cpu'
        return model
    
    def _load_hybrid_large(self, model_type: str, model_dir: str) -> FireRedAsr:
        """æ··åˆå¤§æ˜¾å­˜æ¨¡å¼"""
        print("ðŸš€ æ··åˆå¤§æ˜¾å­˜æ¨¡å¼")
        
        model = FireRedAsr.from_pretrained(model_type, model_dir)
        model.model.cuda()  # å…¨éƒ¨æ”¾ GPU
        
        self.current_device = 'cuda'
        return model
    
    def _load_cpu_primary_gpu_assist(self, model_type: str, model_dir: str) -> FireRedAsr:
        """CPUä¸ºä¸»ï¼ŒGPUè¾…åŠ©æ¨¡å¼ï¼ˆå¤§æ˜¾å­˜ï¼‰"""
        print("ðŸ’» CPUä¸ºä¸»ï¼ŒGPUè¾…åŠ©æ¨¡å¼")
        
        model = FireRedAsr.from_pretrained(model_type, model_dir)
        
        if model_type == "llm":
            # LLM ä¸»ä½“åœ¨ CPUï¼Œä»…ç¼–ç å™¨åœ¨ GPU
            if hasattr(model.model, 'encoder'):
                try:
                    model.model.encoder.cuda()
                    print("âœ… ç¼–ç å™¨å·²æ”¾ç½®åˆ° GPU")
                except Exception as e:
                    print(f"âš ï¸ ç¼–ç å™¨æ”¾ç½® GPU å¤±è´¥: {e}")
                    model.model.encoder.cpu()
            
            # LLM ä¸»ä½“ä¿æŒåœ¨ CPU
            if hasattr(model.model, 'llm'):
                model.model.llm.cpu()
                print("âœ… LLM ä¸»ä½“ä¿æŒåœ¨ CPU")
        else:
            # AED æ¨¡å¼ï¼Œè€ƒè™‘éƒ¨åˆ†æ”¾ GPU
            try:
                if hasattr(model.model, 'encoder'):
                    model.model.encoder.cuda()
                # å…¶ä»–éƒ¨åˆ†ä¿æŒ CPU
                if hasattr(model.model, 'decoder'):
                    model.model.decoder.cpu()
            except:
                # å¦‚æžœå¤±è´¥ï¼Œå…¨éƒ¨å›žé€€åˆ° CPU
                model.model.cpu()
        
        self.current_device = 'mixed'
        return model
    
    def _load_cpu_primary_minimal_gpu(self, model_type: str, model_dir: str) -> FireRedAsr:
        """CPUä¸ºä¸»ï¼Œæœ€å°GPUä½¿ç”¨æ¨¡å¼"""
        print("ðŸ’» CPUä¸ºä¸»ï¼Œæœ€å°GPUä½¿ç”¨æ¨¡å¼")
        
        model = FireRedAsr.from_pretrained(model_type, model_dir)
        
        # å…¨éƒ¨åœ¨ CPUï¼Œåªåœ¨éœ€è¦æ—¶ä¸´æ—¶ä½¿ç”¨ GPU
        model.model.cpu()
        print("âœ… æ¨¡åž‹ä¸»ä½“åœ¨ CPUï¼ŒGPU ä»…ç”¨äºŽä¸´æ—¶åŠ é€Ÿ")
        
        self.current_device = 'cpu'
        return model
    
    def _smart_llm_placement(self, model, available_memory: float):
        """æ™ºèƒ½ LLM å±‚æ”¾ç½®"""
        try:
            if hasattr(model.model, 'encoder'):
                model.model.encoder.cuda()
            if hasattr(model.model, 'encoder_projector'):
                model.model.encoder_projector.cuda()
            
            # æ ¹æ®æ˜¾å­˜å¤§å°å†³å®š LLM å±‚æ”¾ç½®
            if hasattr(model.model, 'llm') and available_memory >= 8:
                # å°è¯•å°†éƒ¨åˆ† LLM å±‚æ”¾åˆ° GPU
                try:
                    model.model.llm.cuda()
                    print("âœ… LLM å±‚å·²æ”¾ç½®åˆ° GPU")
                except Exception as e:
                    print(f"âš ï¸ LLM å±‚æ”¾ç½® GPU å¤±è´¥ï¼Œå›žé€€åˆ° CPU: {e}")
                    model.model.llm.cpu()
            else:
                model.model.llm.cpu()
                
        except Exception as e:
            print(f"âš ï¸ æ™ºèƒ½æ”¾ç½®å¤±è´¥: {e}")
            # å›žé€€åˆ°ä¿å®ˆç­–ç•¥
            model.model.cpu()
    
    def _get_available_gpu_memory(self) -> float:
        """èŽ·å–å¯ç”¨ GPU æ˜¾å­˜ï¼ˆGBï¼‰"""
        if not torch.cuda.is_available():
            return 0.0
        
        try:
            torch.cuda.empty_cache()
            device = torch.cuda.current_device()
            total_memory = torch.cuda.get_device_properties(device).total_memory
            allocated_memory = torch.cuda.memory_allocated(device)
            available_memory = (total_memory - allocated_memory) / (1024**3)
            return max(0, available_memory - 0.5)  # ä¿ç•™ 0.5GB ç¼“å†²
        except:
            return 0.0
    
    def _print_memory_usage(self):
        """æ‰“å°å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        print("\nðŸ’¾ å†…å­˜ä½¿ç”¨æƒ…å†µ:")
        
        # ç³»ç»Ÿå†…å­˜
        import psutil
        memory = psutil.virtual_memory()
        print(f"   ç³»ç»Ÿå†…å­˜: {memory.percent:.1f}% ({memory.used / (1024**3):.2f} GB / {memory.total / (1024**3):.2f} GB)")
        
        # GPU æ˜¾å­˜
        if torch.cuda.is_available():
            try:
                device = torch.cuda.current_device()
                allocated = torch.cuda.memory_allocated(device) / (1024**3)
                reserved = torch.cuda.memory_reserved(device) / (1024**3)
                total = torch.cuda.get_device_properties(device).total_memory / (1024**3)
                
                print(f"   GPU æ˜¾å­˜: {allocated:.2f} GB åˆ†é…, {reserved:.2f} GB ä¿ç•™, {total:.2f} GB æ€»è®¡")
                print(f"   æ˜¾å­˜ä½¿ç”¨çŽ‡: {(allocated / total) * 100:.1f}%")
            except:
                print("   GPU æ˜¾å­˜: æ£€æµ‹å¤±è´¥")
    
    @contextmanager
    def temporary_gpu_mode(self):
        """ä¸´æ—¶ GPU æ¨¡å¼ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if not self.model or not torch.cuda.is_available():
            yield
            return
        
        original_device = self.current_device
        
        try:
            # ä¸´æ—¶ç§»åŠ¨åˆ° GPU
            if hasattr(self.model.model, 'encoder') and original_device == 'cpu':
                self.model.model.encoder.cuda()
                if hasattr(self.model.model, 'encoder_projector'):
                    self.model.model.encoder_projector.cuda()
            
            yield
            
        finally:
            # æ¢å¤åŽŸå§‹çŠ¶æ€
            if original_device == 'cpu':
                if hasattr(self.model.model, 'encoder'):
                    self.model.model.encoder.cpu()
                if hasattr(self.model.model, 'encoder_projector'):
                    self.model.model.encoder_projector.cpu()
                torch.cuda.empty_cache()
    
    def optimize_for_inference(self):
        """ä¸ºæŽ¨ç†ä¼˜åŒ–æ¨¡åž‹"""
        if not self.model:
            return
        
        print("âš¡ ä¼˜åŒ–æ¨¡åž‹ä»¥è¿›è¡ŒæŽ¨ç†...")
        
        # è®¾ç½®è¯„ä¼°æ¨¡å¼
        self.model.model.eval()
        
        # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        for param in self.model.model.parameters():
            param.requires_grad = False
        
        # æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # åžƒåœ¾å›žæ”¶
        gc.collect()
        
        print("âœ… æ¨¡åž‹ä¼˜åŒ–å®Œæˆ")
    
    def get_transcribe_config(self) -> Dict:
        """èŽ·å–è½¬å½•é…ç½®"""
        base_config = {
            "use_gpu": self.strategy['use_gpu'] and torch.cuda.is_available(),
            "batch_size": self.strategy.get('batch_size', 1)
        }
        
        if hasattr(self.model, 'asr_type') and self.model.asr_type == "aed":
            base_config.update({
                "beam_size": 3,
                "nbest": 1,
                "decode_max_len": 0,
                "softmax_smoothing": 1.25,
                "aed_length_penalty": 0.6,
                "eos_penalty": 1.0
            })
        else:  # llm
            base_config.update({
                "beam_size": min(3, self.strategy.get('batch_size', 1)),  # å°æ˜¾å­˜æ—¶å‡å°‘ beam size
                "decode_max_len": 0,
                "decode_min_len": 0,
                "repetition_penalty": 3.0,
                "llm_length_penalty": 1.0,
                "temperature": 1.0
            })
        
        return base_config


def create_smart_loader(hardware_manager: HardwareManager = None) -> SmartModelLoader:
    """åˆ›å»ºæ™ºèƒ½æ¨¡åž‹åŠ è½½å™¨"""
    if hardware_manager is None:
        from .hardware_manager import get_hardware_manager
        hardware_manager = get_hardware_manager()
    
    return SmartModelLoader(hardware_manager)